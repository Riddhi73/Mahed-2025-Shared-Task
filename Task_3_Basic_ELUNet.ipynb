{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTusSKRia9Qb"
      },
      "source": [
        "# **Pip Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_HzON5ukWQP",
        "outputId": "3f5757bd-98bb-46e6-b511-0ebb15d36117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: diffusers[torch] in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (8.7.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from diffusers[torch]) (1.10.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->diffusers[torch]) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers[torch]) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn pandas numpy torch transformers pillow joblib emoji regex diffusers[\"torch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbFgP-MFbCZy"
      },
      "source": [
        "# **Google Drive Initialize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcKWxexHldF",
        "outputId": "a533da7b-1355-4e78-fa68-6f98ef9a0217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK78KZtpbH-s"
      },
      "source": [
        "# **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d0n6cQwRnktS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\n",
        "import torchvision.transforms as torch_transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.serialization as torch_serialization\n",
        "from PIL import Image\n",
        "import os\n",
        "import joblib\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "from emoji import is_emoji\n",
        "from collections import Counter\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline, EulerDiscreteScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U7Fs9IDbTUP"
      },
      "source": [
        "# **Result Ouput Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x1S0BusWBsEX"
      },
      "outputs": [],
      "source": [
        "output_dir = '/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff'  # Directory to save metrics and predictions\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnGEr9Z3bW78"
      },
      "source": [
        "# **Text Preprecessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oVIHtOWekd0Y"
      },
      "outputs": [],
      "source": [
        "# 1. Load the Datasets\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zL4JRPX1ywhA"
      },
      "outputs": [],
      "source": [
        "train_df = load_jsonl('/content/drive/MyDrive/Prop2Hate-Meme/arabic_hateful_meme_train.jsonl')\n",
        "dev_df = load_jsonl('/content/drive/MyDrive/Prop2Hate-Meme/arabic_hateful_meme_dev.jsonl')\n",
        "test_df = load_jsonl('/content/drive/MyDrive/Prop2Hate-Meme/arabic_hateful_meme_test.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'data/' prefix to full ID path\n",
        "test_df['id_normalized'] = test_df['id'].apply(lambda x: f\"data/{x}\")"
      ],
      "metadata": {
        "id": "Kh-LoRO2z2K3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "shZjocdUz92P",
        "outputId": "37a7f5e0-8b2f-4a83-b145-87171ca8ec27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    id  \\\n",
              "0    data/arabic_memes_fb_insta_pinterest/Pinterest...   \n",
              "1    data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "2    data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "3    data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "4    data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "..                                                 ...   \n",
              "601  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "602  data/arabic_memes_fb_insta_pinterest/Pinterest...   \n",
              "603  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "604  data/arabic_memes_fb_insta_pinterest/Pinterest...   \n",
              "605  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "\n",
              "                                                  text  \\\n",
              "0    لما اصحي الصبح مليش نفس افطر .\\nجايلكوا يشويه ...   \n",
              "1    اكثد مخلوقين كسلآ فر العالم ينظران الى بعضهما\\...   \n",
              "2    شايف خلق جديده في المسجد 7٥ SARCAS SOGB\\nواضح ...   \n",
              "3    امي لمت كل الهدوم اللي في البيت عشان تغسلها وم...   \n",
              "4    Hossafassaم\\nانا مهربشي 0\\nالف جثبج\\nكم كومنت ...   \n",
              "..                                                 ...   \n",
              "601  - يابنتي اللي في سنك فاتحين بيوت وعندهم عيال !...   \n",
              "602  صور فتيات الليل فى كندا سنة 1940\\n605\\n3٤\\n05م...   \n",
              "603            -أنتي بتروحي تعملي إيه ف الكليه \\nأنا :   \n",
              "604                        الدكتور قاله ابعد عن الشيشة   \n",
              "605          لما افتكراني متوضيتش وانا ف نص الميك اب..   \n",
              "\n",
              "                                              img_path  hate_label  \\\n",
              "0    ./data/arabic_memes_fb_insta_pinterest/Pintere...           0   \n",
              "1    ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "2    ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "3    ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "4    ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "..                                                 ...         ...   \n",
              "601  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "602  ./data/arabic_memes_fb_insta_pinterest/Pintere...           1   \n",
              "603  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "604  ./data/arabic_memes_fb_insta_pinterest/Pintere...           0   \n",
              "605  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "\n",
              "                                         id_normalized  \n",
              "0    data/data/arabic_memes_fb_insta_pinterest/Pint...  \n",
              "1    data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "2    data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "3    data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "4    data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "..                                                 ...  \n",
              "601  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "602  data/data/arabic_memes_fb_insta_pinterest/Pint...  \n",
              "603  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "604  data/data/arabic_memes_fb_insta_pinterest/Pint...  \n",
              "605  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "\n",
              "[606 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c4af5e3-7550-4759-96b3-8e250f5d11d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>img_path</th>\n",
              "      <th>hate_label</th>\n",
              "      <th>id_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Pinterest...</td>\n",
              "      <td>لما اصحي الصبح مليش نفس افطر .\\nجايلكوا يشويه ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Pintere...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Pint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>اكثد مخلوقين كسلآ فر العالم ينظران الى بعضهما\\...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>شايف خلق جديده في المسجد 7٥ SARCAS SOGB\\nواضح ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>امي لمت كل الهدوم اللي في البيت عشان تغسلها وم...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>Hossafassaم\\nانا مهربشي 0\\nالف جثبج\\nكم كومنت ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>- يابنتي اللي في سنك فاتحين بيوت وعندهم عيال !...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Pinterest...</td>\n",
              "      <td>صور فتيات الليل فى كندا سنة 1940\\n605\\n3٤\\n05م...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Pintere...</td>\n",
              "      <td>1</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Pint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>-أنتي بتروحي تعملي إيه ف الكليه \\nأنا :</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Pinterest...</td>\n",
              "      <td>الدكتور قاله ابعد عن الشيشة</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Pintere...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Pint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>لما افتكراني متوضيتش وانا ف نص الميك اب..</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>606 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c4af5e3-7550-4759-96b3-8e250f5d11d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c4af5e3-7550-4759-96b3-8e250f5d11d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c4af5e3-7550-4759-96b3-8e250f5d11d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-61392afb-f20c-4a0d-86cb-c3c7f8f658f9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61392afb-f20c-4a0d-86cb-c3c7f8f658f9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-61392afb-f20c-4a0d-86cb-c3c7f8f658f9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ac3b09c7-c11c-4100-89a0-3b2fae1000a3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ac3b09c7-c11c-4100-89a0-3b2fae1000a3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 606,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\",\n          \"data/arabic_memes_fb_insta_pinterest/Facebook/images/nasseralkhlify/38391701_1785993401497141_7350648669529440256_n.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"\\u0645\\u0628\\u062a\\u0633\\u0623\\u0644\\u0634 \\u0644\\u064a\\u0647 \\u061f!\\n\\u0639\\u0634\\u0627\\u062b \\u0627\\u0646\\u062a \\u0643\\u0648\\u064a\\u0633  \\u0648\\u0627\\u0646\\u0627 \\u0643\\u0648\\u064a\\u0633 \\u0648\\u0627\\u0646\\u062a \\u064a\\u0627 \\u0631\\u0628 \\u062f\\u0627\\u064a\\u0645\\u0627 \\u0648 \\u0627\\u0646\\u0627 \\u064a\\u0627 \\u0631\\u0628 \\u062f\\u0627\\u064a\\u0645\\u0627 \\u0648\\u0627\\u062d\\u0646\\u0627 \\u0627\\u0644\\u0627\\u062a\\u0646\\u064a\\u0646 \\u0631\\u0628\\u0646\\u0627 \\u064a\\u062e\\u0644\\u064a\\u0646\\u0627\",\n          \"\\u0627\\u0644\\u0644\\u0649 \\u0645\\u064a\\u0633\\u062a\\u062d\\u0645\\u0644\\u0646\\u064a\\u0634 \\u0648\\u0627\\u0646\\u0627 \\u0628\\u0627\\u0644\\u0627\\u0633\\u062f\\u0627\\u0644 \\u0645\\u064a\\u0633\\u062a\\u0647\\u0644\\u0646\\u064a\\u0634 \\u0628\\u0627\\u0644\\u0637\\u0642\\u0645 \\u0627\\u0644\\u0645\\u062a\\u0634\\u0627\\u0644\",\n          \"=\\u062c\\u0645\\u0627\\u0644 \\u0639\\u0628\\u062f \\u0627\\u0644\\u0646\\u0627\\u0635\\u0631 \\u064a\\u0634\\u062e * \\u0639\\u062f\\u062f \\u0627\\u0644\\u062d\\u0644\\u064a\\u0645 \\u062d\\u0627\\u0641\\u0638\\n\\u0635\\u0648\\u0631\\u0629 \\u0635\\u0648\\u0631\\u0629 \\u0635\\u0648\\u0648\\u0642 \\u0643\\u0644\\u0646\\u0627 \\u0643\\u062f\\u0647 \\u0639\\u0627\\u064a\\u0632\\u064a\\u0646 \\u0635\\u0648\\u0631\\u0629\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"./data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"./data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\",\n          \"./data/arabic_memes_fb_insta_pinterest/Facebook/images/nasseralkhlify/38391701_1785993401497141_7350648669529440256_n.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hate_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_normalized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"data/data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"data/data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: count number of data in train_df\n",
        "\n",
        "print(f'Train dataset size: {len(train_df)}')\n",
        "print(f'Test dataset size: {len(test_df)}')\n",
        "print(f'Dev dataset size: {len(dev_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjpJkhPyV4NE",
        "outputId": "0194b48d-83da-4c8b-bfe0-63a5dc0a5145"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 2143\n",
            "Test dataset size: 606\n",
            "Dev dataset size: 312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "-YzDT4-Oxm7i",
        "outputId": "878a2e75-01ad-4a47-ceed-6a711087d038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  id  \\\n",
              "0  data/arabic_memes_fb_insta_pinterest/Pinterest...   \n",
              "1  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "2  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "3  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "4  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "5  data/arabic_memes_fb_insta_pinterest/Pinterest...   \n",
              "6  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "7  data/arabic_memes_fb_insta_pinterest/Facebook/...   \n",
              "8  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "9  data/arabic_memes_fb_insta_pinterest/Instagram...   \n",
              "\n",
              "                                                text  \\\n",
              "0  لما اصحي الصبح مليش نفس افطر .\\nجايلكوا يشويه ...   \n",
              "1  اكثد مخلوقين كسلآ فر العالم ينظران الى بعضهما\\...   \n",
              "2  شايف خلق جديده في المسجد 7٥ SARCAS SOGB\\nواضح ...   \n",
              "3  امي لمت كل الهدوم اللي في البيت عشان تغسلها وم...   \n",
              "4  Hossafassaم\\nانا مهربشي 0\\nالف جثبج\\nكم كومنت ...   \n",
              "5             وانا في السجن بعد ما قتلت وزير التعليم   \n",
              "6  *لما يكلمك يصالحك بعد ما بدأتي تكراشي ع حد احل...   \n",
              "7  عندما يصرح أحد المعاقين ذهنيا أن العلمانية أفض...   \n",
              "8                          «غمض عينيه من اللي بيحصله   \n",
              "9  _الصحاب لما يجو يتصورو بيتصورو صور حلو ٥ *انا ...   \n",
              "\n",
              "                                            img_path  hate_label  \\\n",
              "0  ./data/arabic_memes_fb_insta_pinterest/Pintere...           0   \n",
              "1  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "2  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "3  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "4  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "5  ./data/arabic_memes_fb_insta_pinterest/Pintere...           1   \n",
              "6  ./data/arabic_memes_fb_insta_pinterest/Instagr...           1   \n",
              "7  ./data/arabic_memes_fb_insta_pinterest/Faceboo...           1   \n",
              "8  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "9  ./data/arabic_memes_fb_insta_pinterest/Instagr...           0   \n",
              "\n",
              "                                       id_normalized  \n",
              "0  data/data/arabic_memes_fb_insta_pinterest/Pint...  \n",
              "1  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "2  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "3  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "4  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "5  data/data/arabic_memes_fb_insta_pinterest/Pint...  \n",
              "6  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "7  data/data/arabic_memes_fb_insta_pinterest/Face...  \n",
              "8  data/data/arabic_memes_fb_insta_pinterest/Inst...  \n",
              "9  data/data/arabic_memes_fb_insta_pinterest/Inst...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4ff16b1-73b1-4da1-b098-4bd43ab266c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>img_path</th>\n",
              "      <th>hate_label</th>\n",
              "      <th>id_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Pinterest...</td>\n",
              "      <td>لما اصحي الصبح مليش نفس افطر .\\nجايلكوا يشويه ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Pintere...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Pint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>اكثد مخلوقين كسلآ فر العالم ينظران الى بعضهما\\...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>شايف خلق جديده في المسجد 7٥ SARCAS SOGB\\nواضح ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>امي لمت كل الهدوم اللي في البيت عشان تغسلها وم...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>Hossafassaم\\nانا مهربشي 0\\nالف جثبج\\nكم كومنت ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Pinterest...</td>\n",
              "      <td>وانا في السجن بعد ما قتلت وزير التعليم</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Pintere...</td>\n",
              "      <td>1</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Pint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>*لما يكلمك يصالحك بعد ما بدأتي تكراشي ع حد احل...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>1</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Facebook/...</td>\n",
              "      <td>عندما يصرح أحد المعاقين ذهنيا أن العلمانية أفض...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Faceboo...</td>\n",
              "      <td>1</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Face...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>«غمض عينيه من اللي بيحصله</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>data/arabic_memes_fb_insta_pinterest/Instagram...</td>\n",
              "      <td>_الصحاب لما يجو يتصورو بيتصورو صور حلو ٥ *انا ...</td>\n",
              "      <td>./data/arabic_memes_fb_insta_pinterest/Instagr...</td>\n",
              "      <td>0</td>\n",
              "      <td>data/data/arabic_memes_fb_insta_pinterest/Inst...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4ff16b1-73b1-4da1-b098-4bd43ab266c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4ff16b1-73b1-4da1-b098-4bd43ab266c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4ff16b1-73b1-4da1-b098-4bd43ab266c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a7e691cc-c758-46bc-ba60-5bd3c06bb828\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7e691cc-c758-46bc-ba60-5bd3c06bb828')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a7e691cc-c758-46bc-ba60-5bd3c06bb828 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 606,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\",\n          \"data/arabic_memes_fb_insta_pinterest/Facebook/images/nasseralkhlify/38391701_1785993401497141_7350648669529440256_n.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"\\u0645\\u0628\\u062a\\u0633\\u0623\\u0644\\u0634 \\u0644\\u064a\\u0647 \\u061f!\\n\\u0639\\u0634\\u0627\\u062b \\u0627\\u0646\\u062a \\u0643\\u0648\\u064a\\u0633  \\u0648\\u0627\\u0646\\u0627 \\u0643\\u0648\\u064a\\u0633 \\u0648\\u0627\\u0646\\u062a \\u064a\\u0627 \\u0631\\u0628 \\u062f\\u0627\\u064a\\u0645\\u0627 \\u0648 \\u0627\\u0646\\u0627 \\u064a\\u0627 \\u0631\\u0628 \\u062f\\u0627\\u064a\\u0645\\u0627 \\u0648\\u0627\\u062d\\u0646\\u0627 \\u0627\\u0644\\u0627\\u062a\\u0646\\u064a\\u0646 \\u0631\\u0628\\u0646\\u0627 \\u064a\\u062e\\u0644\\u064a\\u0646\\u0627\",\n          \"\\u0627\\u0644\\u0644\\u0649 \\u0645\\u064a\\u0633\\u062a\\u062d\\u0645\\u0644\\u0646\\u064a\\u0634 \\u0648\\u0627\\u0646\\u0627 \\u0628\\u0627\\u0644\\u0627\\u0633\\u062f\\u0627\\u0644 \\u0645\\u064a\\u0633\\u062a\\u0647\\u0644\\u0646\\u064a\\u0634 \\u0628\\u0627\\u0644\\u0637\\u0642\\u0645 \\u0627\\u0644\\u0645\\u062a\\u0634\\u0627\\u0644\",\n          \"=\\u062c\\u0645\\u0627\\u0644 \\u0639\\u0628\\u062f \\u0627\\u0644\\u0646\\u0627\\u0635\\u0631 \\u064a\\u0634\\u062e * \\u0639\\u062f\\u062f \\u0627\\u0644\\u062d\\u0644\\u064a\\u0645 \\u062d\\u0627\\u0641\\u0638\\n\\u0635\\u0648\\u0631\\u0629 \\u0635\\u0648\\u0631\\u0629 \\u0635\\u0648\\u0648\\u0642 \\u0643\\u0644\\u0646\\u0627 \\u0643\\u062f\\u0647 \\u0639\\u0627\\u064a\\u0632\\u064a\\u0646 \\u0635\\u0648\\u0631\\u0629\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"./data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"./data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\",\n          \"./data/arabic_memes_fb_insta_pinterest/Facebook/images/nasseralkhlify/38391701_1785993401497141_7350648669529440256_n.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hate_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_normalized\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 606,\n        \"samples\": [\n          \"data/data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/bfff148552af71e620ca2a816935ba7d.jpg\",\n          \"data/data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_image_august13/www.pinterest.com_pin_1054827543956207502/58151396cf756d2aba59a4747086be76.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BNpuXkj4D1wS"
      },
      "outputs": [],
      "source": [
        "# 3. Function to Count and Identify Special Characters\n",
        "def count_special_chars(text_series, dataset_name, stage):\n",
        "    # Define special characters: punctuation, emojis, and non-alphanumeric (excluding Arabic)\n",
        "    arabic_chars = r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]'  # Arabic Unicode range\n",
        "    special_chars = []\n",
        "    for text in text_series:\n",
        "        for char in text:\n",
        "            # Check if char is not alphanumeric, not Arabic, and either punctuation or emoji\n",
        "            if (not char.isalnum() and not re.match(arabic_chars, char) and\n",
        "                (char in string.punctuation or is_emoji(char))):\n",
        "                special_chars.append(char)\n",
        "\n",
        "    # Count occurrences and unique characters\n",
        "    char_counts = Counter(special_chars)\n",
        "    total_count = sum(char_counts.values())\n",
        "    unique_chars = sorted(char_counts.keys())\n",
        "\n",
        "    # Save results to file\n",
        "    with open(os.path.join(output_dir, f'{dataset_name}_special_chars_{stage}.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"{dataset_name} Special Characters ({stage} cleaning):\\n\")\n",
        "        f.write(f\"Total Special Characters: {total_count}\\n\")\n",
        "        f.write(f\"Unique Special Characters: {unique_chars}\\n\")\n",
        "        f.write(\"\\nCharacter Counts:\\n\")\n",
        "        for char, count in char_counts.items():\n",
        "            f.write(f\"'{char}': {count}\\n\")\n",
        "\n",
        "    print(f\"\\n{dataset_name} ({stage} cleaning):\")\n",
        "    print(f\"Total Special Characters: {total_count}\")\n",
        "    print(f\"Unique Special Characters: {unique_chars}\")\n",
        "\n",
        "    return total_count, unique_chars, char_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i11zCNSfr093",
        "outputId": "4acbabf3-63b0-401b-d923-ca4214d9b8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing special characters before cleaning...\n",
            "\n",
            "train (before cleaning):\n",
            "Total Special Characters: 3012\n",
            "Unique Special Characters: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '✋', '💔', '🔞', '😂', '😅', '😪', '😳', '🙄']\n",
            "\n",
            "dev (before cleaning):\n",
            "Total Special Characters: 343\n",
            "Unique Special Characters: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '=', '>', '?', '@', ']', '_', '~', '🤯']\n",
            "\n",
            "test (before cleaning):\n",
            "Total Special Characters: 832\n",
            "Unique Special Characters: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '?', '@', '[', ']', '^', '_', '`', '|', '}', '🌈', '🏳']\n",
            "Emojis preserved in: زوجة ماكرون تصرح أن الحجاب يرعب ويخيف الأطفال😅😂😂\n",
            "Emojis preserved in: انا حاليًا بعيش فتره اسمها انا فيا اللي مكفيني وبتعامل مع الناس والعلاقات بمبدأ كونوا زي ما تكو نوا بس المهم بعيد عني✋️\n",
            "Emojis preserved in: الجماعة إللي جربو الجواز🙄 في هزار وضحك وضرب بالمخدات وريش نعام ولا اوشاعات😂😂\n",
            "Emojis preserved in: قنبله نيقازاكي😳 قنبله قطع الاوكسجين\n",
            "Emojis preserved in: ليه مدمنين البيبسي عندهم اكتئاب دايماً ؟ عشان حياتهم كلها soda 😂\n",
            "Emojis preserved in: مل7د جمال الاشموري خالد الرويشان يوتيوبر🔞 احمد الجيشي كمال طماح ابو حطب فواز التعكري الدولة مصطفى المومري\n",
            "Emojis preserved in: وادى قرار جمهورى بتغيير اسم قلعة محمل على ومنع مسرحية محمد على وإغلاق شارع محمد على والقبض ع اى واحد اسمه محمد على😂😂😂😂\n",
            "Emojis preserved in: اشياء عندما تضيع يصبح من المستحيل إستعادتها😪\n",
            "Emojis preserved in: الناس اللي بتنسي😂💔 الو يا ماما كنتى عايز ربطة بقدونس ولا كزبرة كيس إريال\n",
            "Emojis preserved in: صنعاءسواد حنش🤯 سواد حنش🤯\n",
            "Emojis preserved in: أكثر علمين ما لهم أي أهمية في الحياة: 🇮🇱🏳️‍🌈\n",
            "\n",
            "Analyzing special characters after cleaning...\n",
            "\n",
            "train (after cleaning):\n",
            "Total Special Characters: 503\n",
            "Unique Special Characters: [':', '✋', '💔', '🔞', '😂', '😅', '😪', '😳', '🙄']\n",
            "\n",
            "dev (after cleaning):\n",
            "Total Special Characters: 58\n",
            "Unique Special Characters: [':', '🤯']\n",
            "\n",
            "test (after cleaning):\n",
            "Total Special Characters: 123\n",
            "Unique Special Characters: [':', '🌈', '🏳']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, [':', '🌈', '🏳'], Counter({':': 121, '🏳': 1, '🌈': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 2. Text Cleaning Function\n",
        "def preprocess_text(text):\n",
        "    # Define punctuation to remove (excluding emoji-related characters)\n",
        "    punctuation = string.punctuation.replace(':', '')  # Preserve colons for emoji handling in CLIP\n",
        "    # Create a regex pattern to match punctuation, excluding emojis\n",
        "    pattern = r'[{}]'.format(re.escape(punctuation))\n",
        "    # Remove punctuation while preserving Arabic text and emojis\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    # Remove excessive whitespace\n",
        "    cleaned_text = ' '.join(cleaned_text.strip().split())\n",
        "    # Verify emojis are preserved\n",
        "    if any(is_emoji(c) for c in cleaned_text):\n",
        "        print(f\"Emojis preserved in: {cleaned_text}\")\n",
        "    return cleaned_text\n",
        "\n",
        "# Analyze Special Characters Before Cleaning\n",
        "print(\"Analyzing special characters before cleaning...\")\n",
        "count_special_chars(train_df['text'], 'train', 'before')\n",
        "count_special_chars(dev_df['text'], 'dev', 'before')\n",
        "count_special_chars(test_df['text'], 'test', 'before')\n",
        "\n",
        "# Apply Text Cleaning to All Datasets\n",
        "train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
        "dev_df['processed_text'] = dev_df['text'].apply(preprocess_text)\n",
        "test_df['processed_text'] = test_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Analyze Special Characters After Cleaning\n",
        "print(\"\\nAnalyzing special characters after cleaning...\")\n",
        "count_special_chars(train_df['processed_text'], 'train', 'after')\n",
        "count_special_chars(dev_df['processed_text'], 'dev', 'after')\n",
        "count_special_chars(test_df['processed_text'], 'test', 'after')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRJS5L5ilFcx",
        "outputId": "91002173-4578-4411-c0bb-2aa91e452b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['لما اصحي الصبح مليش نفس افطر جايلكوا يشويه رفيعين', 'اكثد مخلوقين كسلآ فر العالم ينظران الى بعضهما Mactتyun 8', 'شايف خلق جديده في المسجد 7٥ SARCAS SOGB واضح ان الامتحانات جايه شديحة', 'امي لمت كل الهدوم اللي في البيت عشان تغسلها ومفاضلش غيرالهدوم اللي انا لابسها عشان تحقق التارجيت بتاع الغساله اقلع', 'Hossafassaم انا مهربشي 0 الف جثبج كم كومنت وناخدها ببلاش ؟', 'وانا في السجن بعد ما قتلت وزير التعليم', 'لما يكلمك يصالحك بعد ما بدأتي تكراشي ع حد احلى ده انت غتت', 'عندما يصرح أحد المعاقين ذهنيا أن العلمانية أفضل حل لمواجهة الإرهاب والتطرف وأنها جائت لمحاربة التعصب الديني والصراع بين الأديان جرائم أتاتورك', '«غمض عينيه من اللي بيحصله', 'الصحاب لما يجو يتصورو بيتصورو صور حلو ٥ انا وصحابى لما نيج نتصور', 'لما تسرح بخيالك وانت بتآكل', 'بالله كيف عرفت اني طالب مصري في كليه هندسة', 'الناس الطبيعيه وهي مسافره انا', 'منهم لله الفراعنة فضلوا يرموا الحلوين فى النيل وسابولنا دول', 'بسجل فويس ممكن محد يتكلم ؟ أهلي :', 'الكمامه ال هتحمينا م الفيروس او هتجيبه مش متاكد', 'لا مؤاخذة فى السؤال لامؤااااخدة فى اييييه فى السؤاااال هو الصيف مطول معانا ؟ اروح انام فى الفريزر طيب و لا ايه ؟ يا جدعااان احنا ماتفقناش ع الصيف ابو فرهدة دا', 'الحمدلله خلصت المنهج ورجعت مرتين احمد انت بتكدب طبعا بكدب الكتاب ضايع مني اساسا', 'فرنسا لموا مليار يورو في يومين علشان ترميم كاتدرائية نوتردام لأ ده احنا نولع في ابو الهول بقي', '١٠ ٠ معم مممعت المحتنم ا ٦و ت د٢ ا1 6fمع 7A717 oda6ne 470و لبنا عل الله', 'صاحبك إنت اللى قولتلى لازم نجتهد عشان ننجح نطلع من الاوائل إنت تقوم تكلم المدرسين عشان نبدأ دروس يا غبي', 'لما مديرك يقولك لازم تشد حيلك ف الشغل تعرف تسكت ؟', 'يا رب ما تحوجنا حوجة فاندام', 'خاف يتعشون وهو يصلي قال أقعد لهم بالمرصاد', 'لما تتخانق مع السواق وتخلي العربية كلها تظيط معاك وبعدين تلاقي الناس كلها بتنزل وأنت نازل الآخر : متزعلش مني', 'خلص صح؟ مش دافع', 'لما اتجوز وولادي يقولي بس احنا يماما مش بنحب الاكل البيتي انا: ولا انا كمان بحبه يلا نطلب بيتزا', 'بعد الساعه ١٢ انا عايزه ارتبط يا جيمي جيمي انا عايز ارتبط؟', 'الواحد نفسه يبقي كويس بس الشيطان هو اللي بينزل عروض ملهاش حل', 'أيوه كلت المانجا الي ف التلاجه وكل اما تجيبو مانجا هاكولها كلها وهوانا باكل المانجا كلها ليه ؟ مش عشان خايفه عليكو من الاملاح', 'المدير المالي وهو بيجري من صاحب الشركة الصعيدي بعد ماقالو الشركة بتخسر ولازم نبيع شوية من الاصول', 'لما الاقي حد بيبصلي جامد إعجاب ولا تنمر ؟', 'الامهات ع ابواب البيوت: هيا ادخلي نجلس قليل ي ام علي والله مستعجله زوجي وعيالي مراعين افعل لهم غداء', 'قدلا تتناول الطعام بسبب كلمه قالها ا حدهم يمزح انا بعد ٣٠٠ كلمه قالها احدهم : يلا بسم الله', 'ومعانا جماعة الإخوان هتحكم مصر تاجل لحر م رت 1 ا ار', 'احمد تعالى اسمع الشيخ بيقولك خدمة الزوجة للزوج مش فرض يا شيخة اقعدي دا انتي يا حامل يا والدة يا تعبانة من يوم م اتجوزنا وكلنا بنخدمك', 'زوجها خانها ويراسل غيرها قامت طبعت المحادثات باوراق A4 عشان مايقدر ينكر', 'انا لسه كارف لخمس بنات النهارده ربنا يزيدك بس مش مهتم بالكلام صد قني تعالي نتكلم في حاجه تانيه اسف شخصيتي خلصت خلاص', 'عاجل السيسي: هنعمل ال250 ألف فصل حتى لوهنلغي العلاوة السنوية للموظفين وانا بقولك هتلفي العلاوة ومش هتبني ال ٢٥٠ الف فصل', 'لما الاقي قلبي مبيتعلقش بحد ومش فارق معاه حاجة وبيضخ الدم بس Me بحبك خد حضن', 'لو سمحتي ممكن تقعدي انتي جنب الشب عشان أنا منقبة وأنا شورقاصة ؟', 'صاحبك الي معاه ايفون يا حلاوتك يا حلاوتك كنت بدور علي صاحب زيك من زمان صورررني', 'مفيش بناطيل اضيق من كدا اضيق من كدا لونى فخادك ازرق وامشي', 'واحد أردني واحد أردني', 'لما اشوف صورة تعجبني واحاول اتصور ذ زيها', 'يابنتي قومي اقعديلك علي الكتاب شويه حاضر ياماما اهو', 'زوجة ماكرون تصرح أن الحجاب يرعب ويخيف الأطفال', 'لما تحل سؤال حادي بادي و يطلع صح خد بالك انا مخي نضيف بس لعبي شوية', 'ويقولك قال داعش عاوزه تدخل مصر ده العيال عندنا في النوبة عاملين التماسيح سكوتر', 'كورونا والعالم بينتهي المصريين :', 'لما تخلص جيش فتلاقيها مستنياك فعلا دي بطلة العالم فالرزالة', 'انا وقع مني ٥٠ جنيه مشوفتهاش ياسطى ؟ متعرفش وقعت منك فين ؟؟ الواد ده غبي ولا ايه', 'دخلت المطبخ وفضلت ساعة جوا واهلي فكروني بعمل الغدا انا لما خرجت :', 'مفيش عندنا فقراء مفيش عندنا عاطلين مفيش صاحب يتصاحب', 'الهاكر الصينى مسابش حد ف حاله', 'بصي انا بحبك اوي وعاوزارتبط بيكي ب مش هنتجوزف الاخريعني عشان متزعليش وتقولي بياع', 'لنفترض ان عدد النسويات 163569463 تجاهلهم كما تجاهلت العدد', 'يا بختك مخلفه بنت والبنات هاديين ومحترمين عن الولاد البنت اللي انا مخلفاها', 'ولاد اختي اذا رحت بدي نام بالغرفة', 'انتي بجد روحك جميله اوي ودمك خفيف ربنا يخليك فعلاً الشكل مش كل حاجة', 'حسستك بالامان اديتك الحنان وحاجات كتير زمان معلش انا عايزاهم دلوقتي', 'أنا سالت عليكي وعرفت ان صوت ضحكتك ممكن يجبلنا بوليس الآداب يا رقااااااصه', 'المطلقة إذا تزوجت تسقط حضانة أولادها ربنا ماقالش كده ربنا عادل ورحيم والرحيم مش ممكن يحرم أم من عيالها', 'لما ازعل ماما وبعدين افتكر كل اللي بتعمله عشاني', 'كل الرجال خائنون طب ماكده كل النساء خائنات برضوهواحنا بنخونكم مع معيز', 'لم تقرر انك هتلتزم الترم التاني وتحضر محاضرات والدكاترة هما اللى يغيبوا انت أنا راجل مهزء اساسا انى حضرت', 'النية : كيك الواقع : صورة اشعة لرئة مدخن', 'ان لوحدك محترم وانت كمان لوحدك محترم انما انتوا الاتنين مع بعض استحاله', 'طلاب كلية الفنون لما حدا يجيبلهم هدية مزيل عرق مش فاهم', 'حكمة اليوم لا تعطي فرص، جرب تعطي بلوك', 'خلاص قومي البسي وغوري مع الصيع بتوعك', '1 يناير 2022 حياتي بقت اجمل بكتير انا حسيت فعلا بالتغيير', 'قاعد لوحدك كدا سرحان شيطان يوزك ف سكه شمال يفضل يقولك ارفع يلا زود يلا', 'انا بعد م اتجوزنا وقاعدين نتغدى في مطعم بليل ولقيت واحده عما له تبصله عشان تلفت نظره وهو معايا', 'ياشمس يا شموس خودى سنه الجاموس وهات سنه العروس 00 sثoشل الم من الذ خربات لايهللى لانا بقول حهلانا الجموسه كده', 'ربنا ما يفضحكم فضيحتي وانا مش معرفه امي اني بمتحن الملحق', 'جمال عبد الناصر يشخ عدد الحليم حافظ صورة صورة صووق كلنا كده عايزين صورة', 'لما تلاقيه كل يوم بيحلو عن اليوم اللي قبله أنا على اخري', 'طب ياعم متزعلش والله ماحد ماسك الدي جي في فرحي غيرك', 'باب الحاره اذا تدبلج بالياباني ايها الاوغاد', 'عملت ايه في الفترة اللي سيبنا بعض فيها، كلمت بنات غيري؟ القلب كان صائما حتى يأذن الله له ازاي وانا شايفاك امبارح مع بنت كنت بتسحر', 'وايه اخبار الصن بلوك ابو ٣٥٠ جنيه', 'ماما0 1 دقايق الاقيكى في البيت', 'تاجيل إنمي الهجوم على العمالقة وبعض الإنميات الا خرى بسبب فيروس كورونا بعد دقيقة: عاجل: شاب يقوم بمحو الصين من الكوكب', 'يعني لما تلبسو صعيدي مش هنعرفكو يعني', 'رئيس الولايات التونيه دونلاند ميا 9 RA ٥ را طلنا', 'يوم العيد لما ترقد الظهر و تقوم المغرب احلامك :', 'لما حد يقعد معايا لاول مره ويلاقيني مبتكلمش بس مكنش ليها لازمة الاجتماعية والرغي اللي علي السوشيال دي', 'كم هي رائعة صور رواد الفضاء فوق القمر', 'لما تاخذ من اخوك الصغير مصاري', 'لما حد يكلمني الصبح بص روح خدلك لفه كده تكون دماغي اشتغلت وافهمك', 'نفس الشي لكنكم تحبون عمرو دياب', 'لما تقوليله انا عايزه بوكس رمضان وتلاقيه باعتلك كرتونه فيها زيت وسكر مع السلامه يا ارق فلاح', 'لما تبقي مستنياه يكلمك ويكلمك شو اللي جابك؟؟', 'قعلا يا يهي دوق العصيد يووه كل شوية كده SCREENSHOT كل شتويه تخلوني ادوق الاكل والعصير كده طعم حلو اوي', 'انا ايموشن ضحك او بقفل كلام في الشات مش مح', 'فاكره لما قولتيلي انا داخله اصالحكم وارجعكم لبعض', 'احتمالية منخفض جوي مصاحب بهطول امطار محافظ عدن أنا في النوم معنديش ياما ارحميني', 'مسمهوش بتاع نسوان اسمه جابر للخواطر ذو مشاعر فياضه تستوعب اكثر من أنثى', 'اضطرت شركه BMW لسحب سياراتها التى تحتوى على صوت إمرأه فى انضمه السياره من السعوديه وذلك لأن السائقين رفضوا تلقى الإرشادات من إمرأه اما فى مصر فقد دخل السائقون فى علاقه معها تقول له : السرعه زائده يرد عليها: خايفه عليا يا روحي', 'لنضع المال جانبا ما الشيء الذي يجعلك سعبدا ؟ الال الذي وضععه جانباً', 'No one: واحدة لسه مخطوبة من 5 دقايق : انتوا سينجل سينجل اوي انتوا ازاي عايشين من غير سند', 'لما ابوك يرجع من صلاه الجمعه ويلاقي البيت كله نايم شاروخان : أنا مسلم وزوجتي هندوسية وأولادي ملحدين', 'كفاية سهر كدا اخش انام يوتيو دب شا هد كيف تصنع الايفون من عصير الجوافة', 'لما اقول نكتة تضحك قدام الثيرابيست وبدل ما يضحك الاقيه بيكتب حاجة', 'عندما تقوم بالتوقف عن استخدام فايسبوك نظرا لسياسة الخصوصية وتقوم باستبداله بموقع تواصل اجتماعي آخر', 'نكته اتقالت من يومين وخلاص ضحكنا عليها واتنست وانا ماشيه فالشارع', 'لما يضل يرفضلك طلباتك ويجب يلمسك', 'مهما كنت اسد ومسيطر وشخصيتك قويه ستحتاج الى قطه تقولك يابيبي', 'الو نايف هلا فخامة الرئيس، الف الف مبروك لنا جميعاً الله بيلعنك يانايف', 'انا في الانترفيو الشغل v5 انا لما اتقبلت After 5 minutes', 'يوتيوبرز القنوات العلمية عندما ينتقدون ليس لك الحق أن تنتقدني فأنا مثقف وقارئ وواعي وعالم من الدرجة الخامسة على مقياس رختر ترجمة الإمام الذهبي لنفسه: سيء الحفظ ليس بالمتقمن ولا بالمتقي سامحه الله تعالى', 'عيلة ابوكي ولاعيلة امك ؟ عيلة جوزي', 'مش مرتبط ولا بحب حد ولا في حد ف حياتي ولا بحد انا برضو لما اسمع اي اغنيه رومانسيه :', 'لما اكون لوحدي ف الصاله وبابا ينادي عليا يقولي تعالي عايزك انا', 'هل تقبلين الزواج بي فى السراء والضراء والحزن والفرح والفقر والغني ؟ نعم، لا، لا، نعم، لا، نعم', 'أكثر علمين ما لهم أي أهمية في الحياة: 🇮🇱🏳️\\u200d🌈', 'لماتدخل الامتحان ومش عارف تحل و تلاقي رئيس اللجنه المعيدين كلهم قاعدين ف وشك نحلننا', 'أيها السادة كان من الشرف سرقتكم', 'راجعة من الجامعة وبسمع زغاريط ف شارعنا أنا : يارب يكون فرحي وعملنهالي مفاجأة', 'ادها بيين اسف اعتقدت انكم مسلمين 7 7', 'تدي أقضي العيد دد في باريس ولا لندن ٦٦', 'الانستا وهويشوفني أفتحه بالدقيقه خمسين مره: روح العب ياخي انت ما عندك حياة غيري؟ خلااااااااااااااااصص الله يرزقك الحياه', 'عملت اى في العيد أنا بلا فخر نايم لليوم التاني علي التوالي', 'أنهي فلانتاين محدش عبرك فيه المصري ولا الاجنبي؟', 'كان ميت في امان الله لحد ماسمع مراته بتقول عمري مازعلته', 'تمثال فى مدخل مستشفى معهد ناصر يرمز لطبيب على هيئة ملاك الرحمة شايل مريض وبيسكعو قلم', 'لما أشوف خبث البنات انا اهلي ربتووني كويس ليييه', 'كل يوم بقوم من النوم مستغرب النهارده بس قومت من النوم مش مستغرب فاستغربت جداااا', 'ولا ف دماغي أصلاً', 'لما تركب تاكسي مع واحد بشنب ونضاره شمس ويقعد يكلمك ف احوال البلد ويقولك انت ايه رايك يا استاذ؟ ربنا يصلح الحال', 'خلاص بقا انا داخلة انام هتوحشني باي هي من الاكونت الفيك انت مرتبط ؟', 'انا مش هقتلك عشان الموت راحة ليك انا هرجعك المقاولين تاني', 'الشعب اليمني المحنك صح في منك كثير بكلية التربية والحاويات حق ميناء عدن ومخزِنين لك بطريقة غلط؟ الشعب اليمني المحنك نترات الأمونيوم والC4 والله مانا داري خلينا مُخزن', 'لما تنزل الشارع وتلقي كل الناس في الشارع وانت بس اللي في الحظر يا ولاد الإيييييه أنتم عايشين حياتكم هنا عادي وسايبين الناس على الفيسبوك عندهم وباء', 'ظاهر الحب تتجلى في هذه الصورة كان بإمكانه أن طلب من ذوجته أن تحمله أيضا لكنه رفض 511157', 'بتقول ايه صاحبتي بتذاكر لا متقلقش دي عليها عفريت مش بيذاكر غير اول صفحة بس', 'هتقاطع المترو وتركب ميكروباص هرفعلك البترين هتقاطع الميكروباص و تقعد في البيت هرفعلك الكهربا هتمرض هنرفعلك الدواء هتقا طع كل ده هرفعك انتا شخصيا', 'لما تكوني اونلاين وانتوا متخانقين ويسألك بتكلمي مين بكلم عرساني ومعجبيني مي سعيد', 'الحمدالله الواحد ربنا هداه وبدا يلتزم ويصلح حياته الي باظت دي بقي', 'بسيوني لو درجة الحرارة زادت نص درجه كمان انا هفرش وانام في التلاجة', 'انا واخواتي واحنا داخلين المطبخ نعمل اكل بعدما بابا و ماما يناموا', 'إحنا إبننا بقي مهندس قد الدنيا ابنهم توووووووووووووت', 'بنفكر ثعمل فيلم ننشر فيه ثهافتنا وتاريخنا للعالم كله هوا دا اللن آعمله Gif؟ تارخنا ططب سس سلام', 'انا داخل نادى كارهي اللغة الفرنسية انا عضو معروف جدا', 'Mood طيب انا اذاكر ولا لأ دلوقتي', '9AM: الناس اللي لسه صاحيه 9AM: الناس اللي لسه صاحيه', 'لما افتح جروب الدفعه والاقيهم بيتناقشو في المنهج وحاجات غريبه كده معرفهاش انا : العلام حلو برضه', 'بوستاتي كلها تقول اني عندي اكس او كراش او مرتبط وانا الحمدلله معنديش التلاته', 'بسكده قصدك بس كد ا ايود بظبكده تمم', 'يوسف الشريف وهو ماسك فيروس كورونا وهاتك يا ضرب شابوه يوسف الشريف', 'وايه حياتي اللي مش نافعه لا دراسيا ولا ماديا ولا عاطفيا ولا حتي فالبيت دي', 'مزجان ث1 وث4 عالم قجانا آ2 58 48874 قاخ حود3', 'لما حد يزعل مني الفتره دي', 'اسمو روستيكا عشان متروحوش تقولوا للراجل عايز من البتاع دة', 'انا قررت ارتبط الارتباط حلو لا بلاش بلاااااااااش خزوووووووووووووق', 'لا مش هرتبط بحد انا اللى عاوزنى يدخل البيت من بابه جايلك عريس لا مش هتجوز جواز صالونات انا معلش', 'مضيفات يقدمن التمر للشيخ حامد بن زايد دئيس ديوان ولي عهد بو ظبي من النظرة واضح انه يحب التمر جِداً', 'د متنرفز م ومش طايق م كلمه انا ت', 'عيل صغير أهله مش حارمينه من حاجه وراحوا زيارة لقرايبهم الله أكل اول مرة اشوف أكل احنا مبناكلش خالص', 'قاعد يبكي على قبر واحد تاني', 'طول اليوم فاضي منتظر صاحب الوايت اول ماتكون مشغول والا مخزن وتتابع مباراة قوية: انا تحت البيت يا غالي', 'فلوس الدرس 500 جنيه ياحاج طب هات رقم المدرس او 17 جنيه ونص مش متاكد', 'لما الاقي خطيبه اخويا كاتبة بوست رومانسي لاخويا وان هو السند والعوض رمضان بطيخة بقي عوض وكمان سند', 'من تضحكلك الدنيا وتصير مشهورمن لا شي', 'وانا بتأكد ان صحابى بيتفرجوا ع المسلسل اللى انا قولتلهم عليه', 'هسا هاي الكندرة مبسوطة ولا أنا بتهيألي', 'تمنيت مرور الايام ونسيت انها عمري مش مهم انا عاوز اقبض', 'افشل ٣ مشاريع في حياتي: الدايت انام قبل الساعه ١٢ احوش فلوس', 'الحين عرفت معنى ممنوع اصطحاب الأطفال بالأعراس', 'الجيش أهلها ظروفك المادية العلقة للى قاعدة في بيت ابوها بتتعلف زي البهايم', 'السواق بيتخانق مع العربيه كلها عشان مفيش فكه انا في الكنبه اللي ورا :', 'امشي ،امشي روحلها مبعرفش امشي بليل لوحدي', 'وقابلته نسيت أني خاصمته الله يرحمها أم كلثوم كانت مدلوقه زيي', 'اتهورى وقصيه حاضر', 'الثغنن فطرولا هيزعل بابا منه', 'إلتمسي لزوجك ٧٠ عذرا انا دلوقتي في ال ٩٢ اقتله ؟ فنجان بدون قهوة سكر', 'اقسم بالله انا حياتي داخل الفندق هذا لابد للقائد ان يشرب مي عشان يكون عقله معه وأرجيله يتحركين وتحية خاصة للملك سلمان قاهر الحوثه', 'لما احط موبايلي على الشاحن والاقي الشحن قل الوادى ٧3305AI80 الوادى هومين بيشحه مين', 'مهما ضاقت بيكي الدنيا متتصوريش بالفلاتر دي Jessica4 Jessica2', 'لما تكون مخزن انت وصاحبك ويتصل صاحبكم التعزي ويقلكم انه ايجى يخزن عندكم : انا الاول تمام وانا بعدك', 'يعني الحواوشي أبو ٣ جنية دد بلدي يا عم جابر؟ بلدي و الله يا أستاذ أحمد أمال أبو ٥ اللي هناك ده إيه ؟ دد بو ليسي', 'يااااه فيديوهات العجول الهربانة هتزيد واحد', 'ماما: لو صحيت بدري هتحس بالانتعاش انا: لما بصحي بدري', 'لما حد يقولي كل سنه وإنت طيبة والسنة الجاية نفرح بيكي لاانا مش عايزكم تفرحوا بيا انا عايز ٢٠٠ جنيه عيديه', 'بابا أنا عايز فلوس عشان أخطب فلوس عشان تخطب ؟ ا٥ عادي أنا متجوز زمك قسط', 'أنا ملاحظ انك بتخلصي كل شغل البيت وانتي ساكته لا ماانا بشتم في سري', 'لما يكون امك، وأم امك ،وميتين امك راضين عنك', 'ابوي: ولدي راح يكون مهندس امي: لا حيصير دكتور انا: محمد النعامي مختص في اخبار الأنمي', 'لما اوقف نضافه عشان الأغاني اللي شغالة مش عجباني', 'ام بسبس تناشداهل الخير يسا عدونها ي علاج ولدها بسبس', 'بكرا لما تتجوزي وتخلفي هتاخدي اللقمه من بقك تديها لعيالك انا حماده حبيب ماما انت عاوز ساندوتش الكفته ده', 'لمجتمع لو لو ميسي خسر كأس العالم', 'لما افتكر اللي كنت ناويه اعمله يوم فرحي يارب سامحني انا كنت هرقص بكتفي بس', 'S هنسوي ذكرا صانة ميمز أمه اصحابي جايين يلعبوا في البيت يلعبوا بلايستيشن صح؟ يلعبوا بلايستيشن صح؟', 'قولهم هتاكلوا فرنسا يعني وزعق فيها', 'لأمتقولش أن النهاردة أخر يوم فى شهر 5', 'واحده لسه متجوزة م 5 تيام وراحت بيت ابوها هي: جماعه المطبخ بتاعكوا فين', 'لما يشلوك الإصلاحيين المسبح 9 انت جاهل فكتبر و تصبح شآب تعلق قلبه بالمسابح و تظهر في سهيل صانة ميمز ؟11 و ٦ر ا HSK', 'والله الامتحانات قربت واحنا هنحل بالهبد صح صح غلط غلط والبقاء للى هبدته اجمد', 'إجري الحقي اخوكي قليله خليهم ٣ أرغفة بس', 'على جثتى لو القطط دى دخلت البيت after 1 week', 'لما تكون نازل بوشنكي او حاويات جورج بولونازلين معك تيمين بيقولك مره مسدس وطاوه', 'فوتوشوب فوتو شوب ياي سبحان الله عصفور و الوجه وجه قط', 'عاجل نتوجه بالشكر لحكومتنا الرشيده على حريه التنفس ومجانيه الاكسجين', 'المسلمين عند الغزو: لاتغدروا لاتمثلوا بالموتى لا تقطعوا شجرة لاتهدموا بناءا لاتقتلوا طفلا ولا كهلا ولا مرأة سمعًا وطاعة الغرب المتحضر عند الغزو: فجروا كل مايتحرك', 'انا في عمر العاشرة بعدما طلبت مني نبع الحنان مساعدتها في غسل الصحون هل انا بنت انا بعد 4 سنوات من وفاتها Bell Arabic Memes ٥١٥٥ ٨٢', 'محدش يتكلم معايا مش طايقة نفسي مالك بس يا قلبي ؟ والله انت اللي قلبي', 'مش قادره اقلكم ريحه كيكه المانجه بالكوسه مجننه طقم التصوير عندي ازاي', 'واحد ظلم كل اللى يعرفهم بوستاته :', 'نمت كتير ، نمت قليل ، منمتش اصلا مليش فيه دماغك تلزمني', 'لا أحد: طالبان لما شافوا كرسي الحكم : إييييه يا دين النبي، إيه الحلاوة دي؟', 'حد بيقولي أخبارك أيه عادي أنا : المنهج متراكم عليا مفيش شغف ولا طاقه ولا فرحه وكمان عندى برد', 'انت كاتب فى الcv بتاعك انك بتتكلم انجليزي ايوه بس محلفتش', 'عندما تمنحك الحياة كل أدوات النجاح و لكنك مصرعلى الفشل', 'لما اكون لسه بحبه بس بخاف من صاحبتي بحبه نعم مبحبوش ا٥ بحسب', 'لما يكون الكل مفكرك رح ترسب و تقوم تنجح بمعدل منيح', 'تقدر تقولي بعد الاسبوعين في ايه ؟ رمضان يعني ميمشيش معاك فاينال يسطا مرحب شهر الصوم مرحب', 'بكر تخلصي الثانوي وتدخلي الجامعه وشك ينور انا لما دخلت الجامعه', 'يالطيب تراك جالس غلط هذي المفروض ورا وش جابها قدام', 'ابوس ايديكم نجحوووني انا دفعتي حواامل', 'لن أتوقف عن مواصلج أحلامى لخعتور وهنام أكترعشان أحم أكتر', 'لما التجار رافعين الأسعار بالشكل ده أومال جهاز حماية المستهلك بيحمينا من إيه بيحمينا من شرور أنفسنا ومن سيئات أعمالنا', 'اخر ماده وهي شيفاني بتقل عليها', '•بس انتي مش هتلاقي حد زيي• ياشيخ الله يحرقك انت واللي زيك', 'شاهد عائلة تحترم النائم ولا تزعجه اثناء نومه', 'يا سيدى لو مضايق انى بخرج كتير ممكن تتجوزنى وتلمنى عادى', 'انا رايح اجيب أغراض من البقالة الي على طريقي عشان رايح ارمي الزبالة ونسيت اطلع الزبالة', 'وانا اللي قولت هيتجوزا في الاخر طلع ثابت علي مبدأه مش بتاع جواز حتي في الاعلانات', 'أنا أدنته أكثر مما فعلتم أنتم', 'شباب بشعر كهذا لديهم إمكانية بنسبة ١٢٥ من سرقة أرضك', 'يا فندم معاذ الله ما يئست لكنها والله والله والله أيام ثقال', 'انا فاشل؟ لول ياحبيبي انا واخد القاب ذي دجعي عنصري ذكوري ادهابي هوموفوبيك FIF٥ EslamMonex', 'ماما انا جعان فر جبنة 9جبنة وعندك جبنة', 'امام مسجد حارتكم وهو رايح يأذن فجر اول الناس:', 'الدكاترة لما يخرجوا مغ بعض ؟ الإكتئاب هياكل منك حتة ياجدع قول ماشاء الله', 'العالم تركيا سوريا اليمن', 'الحصول على فتاة وجهي القبيح وإكتائبي', 'لسه ناوى على الرحيل ارحل يا طارق ينعل ميتين فلنتك', 'مصدلاروى التعبنة والإحصا مم الثالثة عربنا في الجوع خلال 2016 مممامممممتممث طمبامصد كابني ٥0 ن الجوء انوا بعع ممكد متهمم', 'ثم نغطي العجينة ونتركها ترتاح لمدة 24 ساعة ياريتني عجينه', 'اقعدي مع العريس ولو مرتحتيش خلاص مرتحتش يا ماما ومين فينا مرتاح مبروك يا قلب امك', 'قسما بالله لوتيجي تحقق معي لأعترفلها اني انا اللي جب السيوف للكفار في غذوة أحد', 'كيف تتخيل شكلك بعد الحلاقة عالعيد', 'هذا فاهم الدنيا صح', 'يسطاا عايزين نروح الجيم بقي جيم؟تصدق وتؤمن بالله؟ انا بكسل أجيب الريموت من علي كرشي حقيقي', 'موسوعه جينيس تختار نسخه المصحف الشريف الموجوده في الرئاسه المصريه كأكثر نسخه تم القسم عليها كذبا منذ ظهور الاسلام', 'اسلوب امرغرضه؟ لا لا متقولش النصح والارشاد', 'لما تقولك كنت هايل يا حبيبي بدل ما تقولك الله ينور يا هندسة', 'ياجماعة محدش يعرف ده في قسم ايه عاوزه ابلغ عن نفسي', 'لما ابوك يقولك افتحلى تلفونك؟ يا نهار اسود نسيت البصمه', 'ينعل ميتشن غسيل حله الملوخيه واللبانه وطبق البيض', 'نبذة مختصرة عن حياتي : GOOD MORNING تمام شكراً', 'امي عملت أكله حلوه وقاعد باكل في أمان الله أمي : شوفو بتجيبوها بكام من برا دي', 'طلبت اسدال اونلاين وتقريبا ده بنصلي بيه ع النبي', 'لما انام كتيد واهلى يصحونى واسألهم بتصحونى ليه : Translate Tweet NourHan fathy حنا ميهمنا ش تصحى لعه احنا يهمناً تصحى 9 خلاص', 'دخلت البروفه اقيس حاجه وقولت لماما متفتحيش عليا الباب ماما بعد ثانيتين ورينى وصلتى لأى الناس اللى فالمحل ماما', 'لما تتخانق مع حد عالفيس وتلحق تعمله بلوك قبل ما يعملك', 'يا سطي إنت عارف الطريق يا عم عيب عليك ااراال 900 »n ٣د5٠15 انب مانا 102 ٨٧١٧٠٧٥١٥ ٧٥١ ب صق ٢٩٧', 'لما يبقي تاعبني وبرضو عاجبني ولاحول ليا ولاقوه', 'حطوا دبا ديب علي الكراسي في باريس عشان يضمنوا الاماكن ماتتمليش يبقي فيه تباعد بين الناس لو حصل كده عندنا هناخد الدباديب ونطلع نجري', 'عندما تكونين اقصر واحدة في الشلة', 'الي لابس اسود شغال مهندس', 'الهكر تم إختراق الهاتف بنجاح', 'أؤيد تعليم الأطفال الثقافة الجنسية وأفكر في تجميد بويضانى ويوم الخميس تقول لجوزها الحاجة عندك في الثلاجة طلع و سخن', 'احط ك كنافة ولا قطايف ؟ كنايف', 'قاعد مش طايق نفسي وصحبي جي يشتكيلي للمره العشرين من نفس الموضوع انت ايه حكايتك ايه حكايتك مقولنا تأقلم', 'عَاقبته المَدرسة بِسبِب رسَمه المُتكرر على الجُدران بعدها حصَل هذآ الطفل على وظَيفة تَزيين جُدران المطَاعم وهو بِعُمر 9 سَنوات', 'الناس عند عمتهم الناس عند خالتهم انا عند عمتي انا عند خالتي', 'لما نتخانق وابقي انا اللي غلطانه ويعدي ساعتين من غيل م نتكلم: ماتبطل قمص بقى وتصالحني', 'الواليدة قاعدة معا الضياف في امان الله وكملتلها الهدرة وما لقاتش واش تحكيلهم الفضائح تاوعي الفضائح تاوعي الفضائح تاوعي', 'يوم فرحي يوم ما يخلص الصيف', 'وأنت بقي مستني تقول رأيك في انهي موضوع لا أنا مستنى المانجا', 'اتتي نمتي ع حسب هتخلوني اعملكوا حاجة ولا جايبلي أكل', 'ماما: في عريس اتقدم لابوكd الف مبروك يابابا', 'لما شوفير التسكي يبلش يفتح مواضيع:', 'لكن اذا بكرة مت لاتصيحون', 'قد يسرق الفقر مالها لكن لا حيلة له بجمالها', 'الراجل صوته اتنبح عمال يقول جامعه جامعه انا وانا رايح اركب : جامعه يسطا؟', '٨ انهيار الريال البمس والسلع العذانية تنمهد ارتفاع ٥٢8 ١٤٢٤٨٧،٥٤١ البالا تلفد هانة ميمز تاني ارتفاع تاني البيض قد الحبة ب 90 ريالآ البانتا بلميد', 'لما تبقى مضايق وحد ييجي يقولك انا زعلان منك في ستين داهية إللي بعده', 'لما اروح ل مستر العربي بعد 6 شهور اجازه انا المستر تعالى بس قوليلى ازاى مبتدا مجرور', 'مش معقول الأسبوع', 'الموف اون ده غريب جدا منه لله بجد عمري ماهفكر ارتبط تاني خليني كده ايوا روقي كدا ومتزعليش شوفيلي اخوكي كدا', 'لما اتفرج ع اللي بيحصل في حياتي هو في إيه', 'رحلتي التي لا تنتهي للبحث عن مدرسة مابش فيها أستاذ من تعز', 'الحكومة: 150 الف جنيه لمن تعضه الكلاب الضاله', 'بلدان العالم تحصل فيها كوراث ومشاكل وبعد فترة تعود الاموى الى طبيعتها اليمن: الحقيقة انا مش لاقي أي دافع للأستقرار', 'البنات طالعه من الامتحان منهاره عشان غلطت ف سؤال انا وصاحبي بعد ماسبنا نص الورقه فاضيه: الدقه ياحب', 'اللى ميستحملنيش وانا بالاسدال ميستهلنيش بالطقم المتشال', 'هو ده حر بجد ولا مصر دخلت النار', ': داخل اليمن : خارج اليمن', 'لو بتحبي الباد بوي ف انا راجع البيت الفجر لو بتحبي الجود بوي ف انا كنت بصلي الفجر خلصانة بإرتباط', 'بيو مر فؤاد 1 hr لو كان يرضيك تفضل كدا قدامي خلاص خليك', 'طلع للفصل قبل لطاج9ل عاحي على فين يا رجوله Rhalaf ahلuةbم', 'صورة نادرة للزوجة الوحيدة في العالم اللي عمرها ما زعّلت زوجها ونكدت عليه', 'الفر ق بين الصورتين 20 سنه تقريبا الصحاب بيفضلوا صحاب', 'suPer ثظ ن الزواج يااخواني مشروع فاااااااشل Pcr ثنا Got Engaged Today هيبقي شبهي وشبهك محه عمري وعمرك', 'Me Me كل حاجه هتبقي أحسن طبعاً و الوقت دة هيعدى وهننبسط ف حياتنا أوماال', 'لما انزل بالعباية والاقيني بتعاكس برضو البسلك ايه طيب', 'يا بنتي كل دة نوم قومي اقعدي معانا شوية انا : في ايه باخد قيلولة يومين كدة', 'حد قالي حاجه ومسمعتهوش ولسه هقوله كنت بتقول ايه عقلي: يعم اضحكله وخلاص انت لسه هتساله بتقول ايه', 'لما يجي لعنا ضيوف وانا قاعد بالغرفة : راحو ولا لس طيب هم وين قاعدين بدي اشرب مي متى بدهم يروحو', 'قعدوا يقولولى انتي خسيتي انتي خسيتي لحد ما فتحت فى الاكل ومش عارفة اقفل يا حسن', 'وأنتى ايه اللى رماكى ٤ المر RuOa RuQo اتزحلت', 'انت داخل تعتذر لابوك عشان اتاخرت يوم الوقفه جاركوا مستغرب انت بتعمل ايه ف بيته', 'لما اتخانق مع البيست فريند ونتقابل ازيك الحمدلله', 'أمي الساعة 6 الصبح fد هاتولملاية دي علشان لغسلها', 'لما تخلص مكالمه مع حد بتحبه', 'الورد البلدي كل الورد', 'اندلاع الحرب العالمية الثالثة الصينيين :', 'بقول ل بابا عايزين نغير الانتريه عشان لو في ضيوف جت ماما : هو هيجي ولا أيه', 'كي تقول صاحبك نروحو نضربوا جرية فوتينق ساعة ويقولك هذا مكان غي ساعة صاحبك بعد دقيقتين تاع جري : سيبون حميد نزيدو دقيقة و نحبسو عندي بروبلام فالكلاوي', 'لما تحضر يوم وتغيب باقي الأسبوع شقينا فبندلع نفسينا', 'لما يفوت عشر دقايق ما شوفش فيهم القطة دي ع التايم لاين إيه يا شباب الترند خلص ولا إيه ؟', 'لما اختي تتكلم وهي نايمه', 'كان نفسي اعرف اتعامل مع الناس عادي واتكلم مع اي حد بطبيعتي ويقولو الاجتماعي راح لاجتماعى جيه', 'الاطفال بيبقى شكلهم جميل وكيو ت اختي الصغيرة :', 'ماشيه ف الشارع عادي المحلات اتفضلي يا ابله في تشكيله حلوه جوه', 'شكلي اذا سويت مصيبه وابوي قال لاخواني جيبووووووووه', 'الجزائر: انا تنقطع عندي الكهرباء لدقايق السودان: دقايق؟انا تنقطع لساعات نيجيريا: ساعات؟انا تنقطع لأيام لعراقسوريا لبناناليمن ياشباب انتم عندكم كهرباء؟', 'مفاجأة في تحقيقات هدير الهادي: 16 جيجا من الفيديوهات الإباحية وجار فحصها 16 جيجا ليه ياحج هو كان واخد اجازة', 'انا من بكره هنزل الجيم وهخس 9اناهن بكره هعمل دليت 9هبطل أكل L4t٤٢٠٠ نبدأ باليوم المفتوح', 'لما اكون مضايقة وحد يعديي من قدامي تعالي اما اعملك ساعه', 'أنت داخل البروفايل عندي بعد موتي بس شوفت ميم بيضحك', 'كيف ينظرلك ابنك بعد ان يقرأ درس التكاثر', 'ازاي تبقي روش وتعجب البنات بسرعه', 'البزر اذا مايبغاك تشيله:', 'لما تقولي رأيك في نقاش في تجمع عائلي أبوكي: انا مربي عيالي كلهم أحسن تربية إلا البنت دي', 'الحياة اللي بتخيلها في عقلي وانا قاعد مع نفسي حياتي الحقيقية مين اللى عايش الحياة دي يالمبي ؟', 'لما تشتري علبه تونه ب ١٨ جينه وتيجي تصفي الزيت فتقع منك', 'لما حد يهتم بيا فجأة ويقولي عامله ايه؟ ليه ؟', 'البنت لما بتكون ناطره دورها عن الكوافير', 'ادعيلي ي ماما ربنا يديك ع قد تعب', 'لما اعدى من قدام محل مشغل اغانى رمضان يا حلاوة التين و القمرالدين الله الله', 'لما اسمع اغنيه بحبها في مكان عام لأ مش قادرة عايزة أرقص', 'اول صورة لإحتفال مبارك بعيد ميلادة ال ٩٠ الحاجة الوحيدة اللى بتتحسن فى البلد هى صحة مبارك', 'لما ابقي ناذل الكليه الساعه الصبح وآحد بيتنفس عادي معمليش حلجه', 'معقول اكون اتولدت عشان ادفع مواصلات و فواتير واقساط وبعدين اموت', 'لما اروح اشتري مايوه هادية غالب ابو ٢٠٠ جنيه', 'ضحكتك فيها كهرباء ٢٠٠ جنيه كهرباء عند الجسمى عشان المحن بتاعه', 'لا أحد : النسويات Feminists : جسدي ثورة وليس عورة', 'اللهم اعني علي عدم قتل عبادك', 'بابا و هو مجمعنا يوم الجمعه علشان يعرفنا فوائد الفاكهه الي جايبها ف التلاجة:', 'شاهد عائله تحترم النائم ولا تزعجه أثناء نومه', 'الحقيثى يا ماما صاحى بطنى هتموتنى وحاسس بسكاكين فيها 48100 56 مثا قولتلك سيب الموبيل من ايدك قبل مآتام', 'شريط أحزاني حد لسا متعرف عليه', 'أمك لما تشوفك داخل المطبخ تتعشي رابع مره', 'طيران اليمنية: مضيفة الطايرة ياعمة مابش متفل؟ خرج من الحمام يفكر فين بينكع الخ حقة تخلس مابش تكييف شل الروتي والعدس ولا خرجت من هانا خلاص يافندم هات الحاصل منظر يجي دوره بالأكل عشان يقلها اكلك عسل', 'انا عملت اكونت fake عالفيس وانا في امريكا واخدت سكرينات للعيال اللي ظاطت وهروح البيت اغير واشرب شاي بلبن وبعدين هوريهم ايام لون شعر شنب أردوغان', 'لسه عارفين بعض انا وهي من ساعة دي قصة حياتي اقعد اسمعها', 'طارق شوقى يفكر ف اضافة سنة رابعه للثانوية العامة بابا انا جاهزة', 'أنت شخص وحيد وحيد بالمعنى الكامل للكلمة لا أحد يستمع للأغاني التي تعشقها لا أحد قد قرأ ما قرأت أنت من كتب لا أحد يضحك على النكات التي تجدها أنت ظريفة لا أحد يريد ما تجيد عمله ولا يتعاملون به ومش كده وبس لأ سوف تعبر إلى الجانب الآخر وسوف ينساك الجميع', 'عايز نظبط يوم نفطر سوا بقى الحالات الجديدة 358 ياعم اقعد ياعم اقعد', 'لاأريد ان اسمع صوت المؤذن في فرنسا ولن أسمعه إذا اصبحت رئيساً للجمهورية مايحتاج من حجم اذانه بيسمع الاذان من مكة', 'أليس المنادي فج اعد العوبيد معصويا ايود DOarUsl VAUU لالفنق 8 لماذا كلما أناديك لاأفكر الاف ضمك', 'يعني انا بعرف افرق بينهم وفاكرني مش هعرف انت متغير ليه ؟', 'ألو انت نايم ولافى الشغل؟ أنا نايم فى الشغل', 'الكراش جي علينا متبينيش انك معجبه بيه اكيد', 'البنت الطويله دائما تشبه الأميرات ، عمرك شوفت أميره قُصيره', 'اذا احد فضفض لي وما قدرت أواسيه:', 'مبتردش عليا ليه وانت بتحارب', 'الحرامي للأحذية Shoes store هو هاد نفسه الى كان يسرق من الجامع لحد ما ربنا كرمه وفتح المحل', 'وماذا وجدتي في العزله وجدت راحه منكم ومن أشكالكم ومن أخباركم السوده', 'Mohamed Henedy OfficialHenedy الله يرحمك يا جدي كنت حرامي وكمان كداب Translate Tweet توللم الصين حلوة', 'انت لما شفت ميم جامد وحبيت تبعته لحد بس مفيش حد', 'هو مافيش حد بيتجوز عندنا ليه يا ماما م هو ما فضلش غيرك ف العيله', 'حبيب العادلى خد براءة فى قضية قتل المتظاهرين وبراءة فى قضية تسخير الجنود وبراءة فى قضية اللو حات المعدنية وبراءة فى قضية الكسب غير المشروع يعنى خرج من ذنوبه كيوم ولدته امه يومبادعللن دا مكنش رايح يتحاكم داكان رايح تمم', 'أخوك بيتكلم محالمة مهمة؟ أيوه يا ماما روح هات إخواتك وتعالى نتخانق جنبه يلا', 'لما اعيط و بعدها بخمس دقايق اشغل اغاني و اقعد اضحك انا طلعت مُتخلفة فعلا انتوا مبتكدبوش', 'لماكل شويه ادخل اعيط في الحمام اطلع بره متجيش تشخ عندى تانى', 'انت بتسالها هي ازاي عارفة عنك معلومات انت مقولتهاش قبل كدة لأي حد وبتقولك مش عارفة ممكن صدفة هي قبل ما تكلمك : امه اسمها ماجدة عندها خمسين سنة مبتحبش عمته صباح و ساب مريم الاكس بتاعته في تاريخ 152022', 'طه المتوكل علاج كورونا سيصدرمن اليمن', 'مديرة المدرسة وهي واقفة في الطابور', 'كان قدامك اسبوع كمل قبل الامتحان ٥٥ مذاكرتش ليه6 معدش بيذاكر ا9 لا لمايتزنق ا99', 'أحلي حاجه ف يوم الجمعه، هي لمه العيله مع بعض', 'لقد لمحت الخازوق ف شخصيتك منذ البداية ولكني غامرت بالصرمحة معك 36فه0و', 'ايه رأيك فى الأمتحان حلو يا دكتوور أمال محلتش حاجة ليه؟ مبعرفش أرد ع كلام الحلو', 'الحكومة : قد نتخذ إجراءات أشد قسوة معقولة هياخدوا مفاتيح البيت ويقفلوا علينا من برة', 'الإعلام والبنات واحد مشهوركل يوم مع واحدة ومقضيها بدون زواج واحد مشهور تزوج شرعي زواجة تانية', 'لما تدفع نص اشتراك الجيم', 'يوم اتحدوا العرب قصفوا اليمن', 'لما اقول لصحابي انا مش هتجوز غير عن حب صحابي ياستي اقعدي هو فى حد يعرفك غيرنا', 'لما تشوفها عم بتركب عجل السيارة بالشقلوب وتسالها اذا بدها مساعدة انشالله مفكرني بعرفش اركبه لحالي عشاني بنت؟', 'د معين عبدالملك سعيد رئيس الوزراء التقيت اليوم بفخامة الرئيس المصري عبد الفتاح وسلمته رساله خطية من أخيه فخامة رئيس الجمهورية عبدربه منصور هادي تتعلق بالعلاقات الثنائية بين البلدين الشقيقين اود ان اعرب لكم رئيس مجلس النواب والاخ رئيس مجلس الوزراء والاخوه المستشارين والاخوه اعضاء مجلس البرلمان', 'صنديد غثاء ساذج يبي شخْصه و ذاته على أحاديث الرسول ﷺ ومواعظ الصحابة وخُطَبهم يبي ذاته على دورات التنمية الفارغة الباهظة وأقوال الفلاسفة وكاتبي الروايات التافهة', 'كيف تجاوزت الامر بهذه السهوله ولم تتحطم ده منظر ناس تمتلك رفاهية الانهيار يابيه أنا مجبر علي التجاوز', 'اذا قالوا لي اهلي انت ما منك فايده: ماذا سأفعل لكم أكثر؟ هل سأنقذ الأرض من غزو الفضائيين؟', 'الدنيا دى تديها طناش تديك إنتعاش تركز معاها توديك الانعاش', 'خليهم يتسلوا خليهم يتشلوا وتخليهم ليه أصلاً', 'mood me تأنيب ضمير توتر وسوسة تردد', 'اية دا انت السمك بقالي 25 سنة عايش ماكو وكل مرة تتفاجئو اني مبحبش السمك', 'وانا قاعدة مستنياه يكلمني لما يروح', 'مين مديرال HR عندكم فالشركة ياكوف بنيامين حانينه', 'ممكن يا دكتور كفايه كويزات SERENSnOT لو مافيهاش اذعاج يعني', 'لما يبقي اونلاين وميكلمكيش دا غراااامه طلع اوهام', 'إنتى معاكى فلو لس لأ خالص طيب إيه بكام بكام اللى عمالة تكتبيها فى كل مكان دى ؟', 'تخرجى معا متبينمن انن معجبد بمداءء 4e 7tn', 'اول ما انتشر فايروس كورونا وقبل لا يوصل للعراق طلعلنا سيف جنان يكول المرض، ما ممكن يوصل للعراق واذا وصل يموت بدرجة الحرارة العدنة معقولة ناجح من السادس بسماعات', 'لما نلعب ببجي ويقولي ماتتحركيش لوحدك خطيبي', 'POV: يما أنا مش رح أتجوز إلا بنت تونسية أو مغربية', 'الحهرباء «تنحد علم المصريين غدا بالإعلدن عن الذسعاد الجديدة توبت بوك مته يند عى ومك ياممج', 'لما تتصوري صوره حلوة وتوريها لابوكي وتلاقيه بيلقب في باقي الصور', 'بالله عليك كيف عرفت اني نايمة معيطة ؟؟', 'شيطان دخل راس مسؤول الصبح وطلع العصر سألوه : ليه طلعت قال يخربيت سنينه دوخني وجابلي جلطة الصبح بيسرق والظهر في الجامع وفي درج مكتبه خمرة وفوق الكتب مصحف وماسك سبحة بيسبح بيها بالنهار ويروح يرقص في الكباريه بليل عليا الحلال لدرجة ما بقيت عارف هو اللي لبسني ولا انا اللي لبسه', 'يابنتي ذاكري ده اخر ترم خليكي تخلصي سيبوني في حااااالي', 'دخلت المحل بقول للراجل عايزه بلوزة بكوم طلعي دي', 'سألها إبنها: لماذا انتي أكبر حجماً من باقي الامهات فخسرت ٨٠ كيلو من أجل إبنها، لو عنا كان أكل كفين ونام زعلان', 'السلام عليكم لو عايز نرجع زي زمان قول للزمان ههبم هات قلب اولا لا داب وثانيا ولا حب وثالثا ولا انجرح ورابعا ولاشاف حرمان', 'الدول العربية تحرير المرأة خلع تفكيك افساد اخلاق الحجاب الاسرة المجتمع نسوية عربية الحركات النسوية الغربية', 'لأ خللى بالك أنا مكتئب ممكن أبهد لك', 'طلعوا الهدوم الى مجتالبسوهاش علشان ثدر يها لناس محتاحاما Pak 9دا مبتلبسهوش ليه ماله ده', 'انا مش منزلك الشاي عشان تنزلي تنقذي اختك اللي غطست تقومي تغطسي معاها', 'لما امي تشم خبر اني عايزه اخس ماما : ماما', 'بسأل امي عامله اكل ايه النهارده عادي أمي: نفسي اللى يتحط قدامك تاكله وتحمد ربنا', 'وانتي لما حد بيقولك كلام حلو بتعملي ايه ؟ ببعتله أستيكر', 'ماشيه فرحانه وبغني عادي في البيت ماما: هو مفرحك أوي كده؟', 'أمور يجهلها الرجال عن النساء دليل إرشادات غسالة الأطباق', 'لا بتعرف تطبخ ولا تغسل ونكد ليل نهار ومسلسلات هندى بال400 حلقة طلقتها', 'أصحاب ذمار كيف يتخيلوا المحيط الهادي', 'أنا مشكلتي مش في التقليد أنا مشكلتي في ريأكشن جميلة عوض', 'قاله تراهني اضرب الشعب تاني هو هو علي قفاه؟ قاله أااااااااااااراهنك', 'لما تبقي راكب ميكروباص كله بنات وتقول للسواق اخرك فين ياسطا السواق متعرفش تجيبه', 'شايفك سيبك من اللي راكبة ورا وسيبك من اللي راكب قدام السواق وسيبك من السواق نفسه مع اني مش عارف هو سايق ولاشايف الطريق ازاي وسيبك كمان من اللي شايلنه خدلي بالك بقي من اللي قاعد تحت السواق ده', 'الشعب اليمني', 'Mohamcd Ayman Abd Elrazek ج٤ الف مبروك يا ذكىع 99 قولى بها ايه ئ الرغبات جب هيهيهى قوله بقا هتكت كليه ابه', 'بعل ما اتخانقنا 60 مرة ف اليوم احنا برضو :', 'لما تجوزي وشقتك تبقى قريبه من بنت اهلك', 'الكوبايات هعملكم عظمة ب٥ جنيه', 'سبب خسارة أعز أصحابك هو مين بدو يقعد قدام بالسيارة ٥yerja', 'لما امشى مشوار٢ كيلو لوحدي أنا هريحلي حبتنين لما امشى لحدود ليبيا مع شخص بحبه عصير قصب ونكمل ؟', 'لما اتصل بصاحبي كرمال ببعتلي الصور يلي صورني ياهم بالعرس :', 'تغديتي؟ لا يروحي مالي نفس اكل', 'وانا قلت هاه يا نور ؟لا ما بلا يا قاضي قالت وطلعنا ترند', 'انتو كاتبين اجابات زي بعض ليه ؟ عشان بناخذ نفس المنهج', 'بالله عليكي كيف عل فتي اني حامل', 'عشان ترضى جوزك لازم تبقى الصبح صافيناز والضهر هيفاء؛ وبالليل نانسى بغض النظر انه طول الوقت بيبقى ابو حفيظة', 'بحبك يا ندي منشن ل ندي عريسها هو', 'وفلخر يجي وااحد رااسوو مربع يقوولك لأروبيات خير وبلا ماكيااج من الجزائريات', 'تقريبا عشان يحافظوا عل اسرع دليفري فب مصر نسيو البرجر 7 McDonalds البرجرياااا ؟ ٤', 'مرضيه يا فاطنه يالهووي ياحج', 'م٧ ينادوا عليا في البيت اعملهم طبق السلطه بتاعي Snapcha', 'الاستايل بتاعي وانا طالعة البلكونة بنشر غسيل بقي موضة', 'روحي اقعدي مع عريسك ومتحسسهوش انك مجنونة حاضر شوف بقى هتتجوز عليا، هتجوز عليك', 'لما اقعد مع نفسي اشو ف عيوبي عشان اصلحها ما فيا عيو ب وكمان قمر خدي بوسة', '١٢٦٨٧٢ بل ٠ اصجاب يافع اصجاب الضالع خطاب الرئيس القائد عيدروس قاسم الزبيدي حول', 'اما تيجي تكتب موضوع تعبير المصحح ما انت كاتب نفس الكلام فالسطر اللي فوقيه يبن المتناكة', 'لما اكون فرحانه وبضحك وبعدها ب ثانيه ارجع مكشره ومش طايقه نفسي ولا حد أنا بتحول تانى يلاا', 'امبارح كنت قاعده في مطعم وفجاه جي شاب حلو اوووي لحد عندي وبصلي وقال لي لوحدك ولا منتظرة حد قولت له لا انا لوحدي قام سحب الكرسي الي جنبي واخده ومشي الهي تسحبك عربية مليانة بقؤ يااابعيد عبو شكلك', 'الاكس جاي عليكي اهو حاولي تبيني انه مش فارق معاكي وانك نستيه', 'الحياة فرص مش عجوة وقرص', 'الكوتيين 2020 :المصريين ليسوا شركائنا فالوطن وهم هنا لخدمتنا الكويتيين 1990: عمي مبار ك الحقني عمي مبارك', 'هيا ارض الواقع منين لو سمحت؟ تاني مصيبة على إيدك اليمين بعد الخازوق الجاي علطول', 'خلتيه يبطل سجاير ازاي ؟ قولتله شفايفك دي انا اولي بيها من السجاير ااااااااااااه', 'لما تتجوزي وتلاقيه تاني يوم بيصحيكي و بيقولك اصحي يا قلبي خلي الشمس تطلع بقي انت مين يا رايق', 'لما اشوف واحدة مرتبطة بتقول للي مرتبطة بيه يا بابا يا مرارتي', 'لما اقعد اصيح طول الوقت انى مش مرتبطه ومفيش حد بيحبنى وألاقي حد بيقولى انا بحبك لا شكرا انا بثير الجدل بس', 'ويبقى الرسم هو المكان الوحيد الذي أهرب إليه عندما اكون وحيدة', 'اخبارك 9اس العاهل السعودي وولي العهد يقدمان التعازي إلم أسرة خاشقجي 0 افام فبراير الا سود ربنا يجازى 9د لحم عرفتوش مين قتله ؟', 'هو : هل تقبلي بي كزوج في : السراء و الضراء و الحزن والغنى والفقر ؟ هي: نعم لا لا نعم نعم لا', 'الدفعه كلها راحت الجا معه انا الساعه 2 الضهر جود مورنينج شاهيندا', 'اللجنه كلها مصدقت إن المراقب خف شدع اللجنه شويه وبدأنا نحل مشرف الدور من الشباك :', 'الشيعي لما يعرف ان فى احجار مشعة ومفيش احجار مسنة', 'الاتراك واللبنايين زودلى توميه يا معلم إمشومه لبلد حرام عليكو أخدتو وظايفنا وقطعتو علينا في لقمه عيشنا المصريين', 'احصائيات قوقل اكثر من يقوم بعمليات النصب والاحتيال في العالم هم الرجال وبنسبة ١٥٥ احلام الدبعي بلقيس الحداد', 'شباب اللي لسا ما خطب لا تزوج ينطر شوي بجوز يكون نصيبك من اللاجئات الاوكرانيات', 'لما صاحبتك تبعتلك بليل بت صاحية آه يا مصايبوو', 'عندما يسأل بوتفليقة عن المدة التي يريد قضاءها رنيسا للجزائر', 'طب أنا بكشف ب80 جنيه زراعة وبيطري والنبي ياتوحه نوليني 8 آلاف جنيه مكسب صفقة الكتاكتيت الأخيرة', 'لما تتركي البيست منيحة بالليل وتنامي وتصحي تلاقيها حاذفه صورة الواتس صباح الخير يحبيبتي لا اله الا الله لا اله الا الله', 'اذا فتحت شركة انا وصاحبي', 'شعبة الملابس الجاهزة زيادة في أسعار ملابس الصيف القادم تصل ل100 هنصيف عريانين كلنا', 'انا غنوبي ولست يمني والموت للدحابيش الغنوب حقنا، والغنوب غادم وبغوة', 'ماعتمشيش هذي غيرها لي هيا', 'أنابصفتي مواطن لا أريد رئيسا يرحل ولا حكومة ترحل اعطوني التأشيره وأنا الذي سوف يرحل', 'ظهور الهلال في مدينة القدس و بذلك يكون يوم غد الجمعة اول ايام عيد الفطر المبارك', 'إثنين حوثيين بعدما سيطروا على أكثر من 70 جبل سعودي طياران التحالف السعودي المدعوم من الNATO أنظر لما يحتاجون اليه ليقلدوا جزء بسيط من قوتنا Look what they need to mimic of fraction of our power', 'البيت كله نايم وصحيوا مخضوضين من صوت الخلاط ومش فاهمين في ايه انا في المطبخ : الكوبايات هعملكم عظمة ب ٥ جنيه', 'حد هيعملكو كده فى الفلانتين و لا اعملكوا شاي بلبن', 'لما تسأل القهوجي علي باسورد الواي فاي ويقولك اتناشر سبعة حداشر خمسة 777777777777 55555555555 12755555555555 127115 777777777777115', 'لما تخطب صالونات وتصحى تاني يوم تلاقيها باعتالك مسدج 10 سطور قدايه انت السند الحقيقي : جايه معاكي ف تقفيل تعبير', 'مساء الخير لو سمحت كنت عايز أعرف إيه القرف ده؟', 'وتعبان ومش قادر اتحرك والجو حر امي : ما تقوم تفكلي الستايراغسلها؟', 'الزوج لما يشوف مراته متنرفزة وبتشد فى شعرها وهى بتذاكر للعيال إنه إنتقام الرب', 'انا واصحابى بعد جلسة طويلة من الحش في الناس بس احنا ملناش دعوه بحد آه استغفر الله', 'قوم ارمي الزبالة حاضر قوم روق معاي البيت حاضر روح هات حاجة من المحل حاضر قوم شيل الغسيل لا وزي مانتوا شايفين ابني مفيش مره قولتلوا علي حاجة وقالي حاضر', 'أخيراً قدرت أقنع أهلي أسافر يومين مع صحابي روتانا سينما: بيقولك مرة فيلم المركب', 'مصر اسرائيل منشغلة بالاحتفال باعيادها في تشرين من عام 1973 سوريا', 'اغتيال شخصية بارزة بعدن : نهئح تحهيق وثعرف مئ الهاتل صانة ميمز مم ذبنبه نحرق سياكل ثار', 'ا تتعرف علي شخص جديد لااستني متقولش م كنت فكئي مفرودومشبر صح', 'مهوأنا عايزأقولك إن الواحد ممكن يطفى المنبه بس مبيبقاش عارف إنه بيطفى المنبه', 'شغلاني مش هقولك تانى Dina', 'اده بوستات البطاط دى كلها', 'الاتنين انا انا عاوزه اتخطب زيهم يبشا فيه عريس متقدملك خدوامنه الجتوه وقولوله مش موافقه', 'كيف النسوان بفكرو الرجال بشوفوهن كيف الرجال بشوفو النساء بلحقيقة', 'شباب المعادي يوزعون دونتس وكنافة بالنوتيلا عل الصا ائمين عسنا مندنا يبشا بيحدفوا لببج هي وشك بالنبلة فتسب الدين فتفطر', 'تعملهم مسلسل الاختيار Le Choix وهبعتلك فيفي عبده تلعب دور مارين لوبان، واحمد عز يلعب دورك وانعام سالوسة تلعب دور مراتك', 'هل صحيح انك ك صيدلي يمني بدئت تعمل ابحاث عن فيروس كورونا ؟ ومثماديخيياشه', 'دلائل تثبت سيطرة عبد الفتاح السيسي والمسلمين على الإقتصاد الأمريكي', 'الامهات بتفرح لما عيالهم يحضنوهم أمي لما أجي أحضنها: من غير تلزيق مش طايقه روحي', 'إختاروا صح عشان العيال اللي بتدفع تمن اختياراكم', 'رئيس الوزراء: فتح المقا هي و الكافيهات و السينمات و المسارح بنسبه اشغال ٢٥ ونراهن علي وعي المواطنين المواطنين هيييييييييييييييييييييييييييه', 'بالظبط لما تذاكدي لإبنك القسمه المطوله ساعات ويقولك احطايه بعد اول خطوة حط طحينه', '9 سنوات انا : مابي اروح المدرسه اليوم بعد كفين :', 'لما تبقى بتبوس خطيبتك وامها تدخل عليكوا فجاه', 'لما تكون خاطب جديد وعمك يحكي نكتة', 'مش ناوي تلم منشوراتك الجامدة دي ؟', 'لما أبقى خلاص بنام وأسمع صوت أطباق في الصالة بتاكلوا إيه', 'بيت فيه حد ثانويه عامه : Abdelrhmon Alod قرايبهم يوم الخميس الجاي', 'كلمني عن احساسك وانت بتلعب بيها وهي بتدعيلك ف صلاتها احساس رائع', 'حزب الإصلاح الي هرب تركيا حزب الإصلاح اللي حنب باليمن استمروا يعطيكم العافيه إبداع والله', 'مالك ؟ مانا فل أهو حساد متدايقة احكي', 'مني الشاذلي و هي مستنية احمد خالد صالح و هنادي لما يرجعومن شهر العسل', 'لما تبقى أجازة تعدى على ذما يلك 9 الشفل', 'السيسي الشعب المصري العريق', 'لما ترجع من ايكيا وتبص علي عفش بيتكو', 'لما اجمع البيت كله علي فيلم والاقي البطل بيقرب من البطله ايه يا اسطا هتكبرلها في ودانها ولا ايه؟', 'كليه ايه اللي بكره انا لسه متخطبتش', 'اوعي تتخلي ابدا عن احلامك', 'لما ادخل ع الاكونت بتاعه وادور ع ريأكت لبنات وملاقيش أخلاقك مش بايظة ليه مستقيم كده ليه يبني', 'انتي فاكره انك لما تبصيلي كدة هركتب هربكت هرتكك هكربت هرتبك يعني الدمم6 نو ٢V 00ق2 00 300 100', 'ماشاءاللة يا حبيبى كبرت انت ابن سعاد ولا ابن مين', '٠ شكرايا صلاح على وطنيتك وحبك الغير محدود لمصر ستبقى مصر عظيمة رغم انف الحاقدين والسفلة والمدعيين وميت غمر الحاقدين السفلة والمدعيين ميت غمر', 'واحدة زيك عندها كل حاجة مكتئبة ليه عشان واحدة زيك نبرت فيها واضحه اهي', 'خطباء مساجد صنعاء وهم يشوفو مواضيع يتكلمو بها في خطبه الجمعة :', 'ريال قطري ريال سعودي ريال عماني ريال يمني', 'حكمة اليوم حتى لو كنت أسد ما تمزح مع حمار', 'تعزي ياكل عصيد بس بدون وزف', 'دكتور من ساعة ما اديتني المنوم وانا مرتاح بتاخد كام حباية؟ لا مش باخد بديه لمراتى', 'المسلم الكيوت لعله أقرب إلى لله منك الشحارير اعتنق الإنسانية الكفار سيدخلون الجنة هل أنتم أوصياء الله في الأرض يا دوغمائيين دعوا الخلق للخالق', 'مهما كان يومك سيء تذكران هذا السائح الاجنبى باعوا له هذه البدلة على اساس انها لباس رجال تقليدي', 'ليست لدي خبرة في الجراد لكن متأكد أنها أنثى', 'وانا عامله نفسي متأثره وانا اصلا اللي سايباه:', 'يجو صحباتك يقولولك الحقى دا بيكلم دي ودي اه طبعا بكلم دي ودي وهو عشاني دا عشانك عشان اخد خبره ادلعك بيها', 'صحابي قمرات مفيش فى ميتين امهم غلطة اللي بيرتبطوا بيهم', 'رمضان دا ولا كيليان مبابى', 'البتاع ده حيدخلني الجيش وربنا كنت بنام وسط ابويا وأمي لحد ما اخدت الاعفا', 'الساعة ٣ الفجر وانت بتقرر انهي الاصعب تقوم تخش الحمام السقعة ولا تنام بزنقتك', 'بنت رجعت متأخرة ٣ دقايق عن ١٠ بالليل أمها: مالسه بدري يرقاصة', 'كيلو الطماطم نكام ؟ الكيلو ونص ب5 والاتنين ب7 كيلو الطماطم بكام ؟', 'الطفلة لوكان تخلى ماماها تلبسلها', 'الشوارع كلها علقت زينه رمضان شارعنا :', 'والله لتيجي معانا والمصحف لتيجي معانا', 'آريحي قبضة يدك فنحن الرجال لانخون فنحن ايه يا عنيا', 'لما بعد ساعة إلا ربع في المحاضرة تزهق فتبص المو بايل تلاقي عدى دقايق بس', 'الحركه دى تقريبا الشعب المصري كله بيكرهها وانا اول واحد', 'مقلع الاثنين ملط، دايس على وش واحد، بيضرب الثانس بمرزبه، مجمع خدمة المواطنين', 'ناطراللحظة المناسبة وين سالم', 'لما حد يعصبني وانا صايمه: اللهم اني صايمه تعالالي بعد الفطار', 'سيدي القاضب ده ترم 24 حصان والعيال نص حمار ي فرق فالسرعات سيادتك', 'أنا خلاص كده مش عايز أي حاجة تاني م الدنيا ماشاء الله حققت كل أحلامك لا بس زهقت', 'لما احط علي وشي الكريم بتاع اختي الي قالتلي اكتد من 100 مرة مجيش جنبو وتسالني وشك منور منغ الل سلبمان على فكره آنا بحب', 'اصحاب صعدة اول مادخلوا صنعاء 2015 ومشوا من شارع هائل: اوووووهوو كوافي نفرين', 'قاعد في البيت مخنوق و مش طايق نفسك ابن اختك خالو شعنئي ٤٥R٥٨؟', 'احنا لازم نسيب بعض والولاد ال انا مسحتهم عشانك ارجعهم ازاي انا دلوقتي', 'أيوه يا فندم حضرتك إحنا بقالنا فتره ف الكلكعه كنتم قايلين إن حيبقى في شخلعه أنا بستفسر بس لتكونوا نسيتونا', 'غلاء معيشة ارتفاع اسعار سوء البنية التحتية الحكومة النواب المواطن السحيج', 'انا والله يابني منا كنت بغلي ولسه فاصل لا اشوفك بتفصل قدامي لامؤاخذه', 'لما حد يقولي ممكن الفيس بوك بتاع حضرتك يارب يفضل يقولي حضرتك بعدما يشوفه', 'انتي يا بنتي بتعرفي عنه كل الحاجات دي إزاي ؟ بالصدفه والله', 'حبيبي داب علي ايدي وتاب يعني مش انتي اللي كرهتيه ف صنف الستات تؤ تؤ داب وتاب', 'كل عام وأنتم بخير صلاة العيد 5:19 صباحاً بتوقيت الجيزة أيوة الساعة 519 يوم إيه؟؟', 'لما اتخطب للي بحبه و تيجي اغنيه بحبك سنين في السر و محدش عرف صحابي والمعازيم وعمو ابو احمد بتاع السوبر ماركت:', 'المنهج مش عايز تتعرف عليا خالص', 'لا أحد : الملحدات بعدما يحصلن على لجوء في الغرب :', 'لما تكون عمال تمشي ورا قلبك و تتخزوق مخك : استخدمناااااااي', 'اتمدوا الد بعطيكم العافبة اذا دخل محل حلويات و أكلت 7 انواع ببلات والموظف نآطر لحنى شتدب 3yeriaw', 'انا جاي لعمي فاندااام', 'مفيش حد راضي يعلمني السواقه اُقسم بالله', 'نذلنا نصلي الفجد والمؤذن خلاص هيبدا وآذا بشخص جي من ودا بيذعق وبيقول للمؤذن استنا استناا فكلنا استفربنا والمؤذن وقف لقيناه داح للكولدير وداح شادب بسدعه وقاله يلا اتفضل آذن يا شيخنا ا 1 م ٥', 'حر وعرق في أول ديسمبر يا عرة الكواكب يابو أوزون مخروم ؟ هو ده الشتا يابو جليد سايح وطقس نايح وفرهدة في الجاي والرايح ،', 'انا هخرج من حياتك عشان انتي أوكيلا إله إلا لله', 'مبتسألش ليه ؟ عشاث انت كويس وانا كويس وانت يا رب دايما و انا يا رب دايما واحنا الاتنين ربنا يخلينا', 'آلا هو لماذا توقفنا عن السقوط هل انتهت احزاننا ؟ لا يا استاذ لمبي لقد وصلنا للقاع جيد لن نسقط مرةً آخري', 'ابعدي عن الراجل يا خطافة الرجالااااو', 'وانا طالع م البيت 9 وربع وبتمني اوصل المحاضرة الساعة 9:', 'أنا مكمل حياتي كنوع من أنواع الفضول اللي هو يا ترى إيه اللي هيحصل تاني ؟', 'ودي احطها فين يا ريس', 'أنا واقف مع الواليدة فالكوزينة وطاح ليها طبسيل من يديها Arabic Memes للواليدة أخرج علياالللل Abis شffoos officiellو CArabic mismne', 'البنات لما عيل صغير يقولهم يا طنط الولاد لما عيل صغير يقولهم يا عمو', 'لما امك تقولك دقيقه و تبقي ف البيت وانت اصلا ف اسكندريه', 'السيسي : كثيرمن اللي بيتكلموا مش شايفين القفزة اللي بتحصل في مصر يا سلام يا شيخ عبيد لو تشوف القفزة اللي انا شايفها', 'حرفياً مشروع تعديل النوم افشل مشروع مر عليّ بحياتي', 'عندما يعم الهدوء في المنزل فأ علم آن هاتفك في آيا دي آمينة', 'لا تصاحبني من اجل المصلحه فأنا شخصيا غيد مستفيد من نفسى', 'دافور الفصل : لما يعرف انكم مسوين قروب للغش :', 'من اول الصيف بصيح وعايزه الشتا يجي بسرعه اول ما الجو بقي في نسمة هوا كونجيستال', 'كل القنوات في رمضان تتزل إعلانات أكل ومشروبات وتموتك جوع قبل الفطور قناة السعيدة :', 'لما اقولهم في البيت اني هبطل آكل كتير وهعمل رجيم امي ابويا اخواتي صاحب السوبر ماركت', 'لما الاقي الناس كلها بقت تحبني بعد ما بقتش اطيق منا كنت قدامكوا السنين اللي فاتت', 'مسميش بتاع نسوان اسمي جابر للخواطر ذو مشاعر فياضه تستوعب اكثر من انثى', 'اليمن السعودية اتفضلو مساعدات تمرسكري درجة إوئ صانة ميمز السعودية تمر ايش ؟ محتاجين خدمات زي لكهرباء9لماء اليمن السعوديةن مشوحالكم بالتمل نوعية كويسةوالله اليمن', 'المطبات فى أوروبا بتتقاس بالسنتيمتر لكن عندنا تطلع على المطب تشوف أم أحمد طابخة إيه فى الدور التانى', 'وزارة التعليم العالي تطلق حملة خليك مستعد لطلبة الطب البشري وطب الاسنان للمساعدة في مواجهة كورونا مش انتوا كنتوا بتقولوا اسنان متقلش عن بشري؟ احنا يا معالي الو زير ؟ ده احنا سباكين', 'لما تبقى فى مدينة نصر المفروض مكناش نطلع الكوبرى يا معلم انت بتقول ايه يا واد ؟', 'المصور مطلوب حالا اللقطه ناااار', 'واخد الاجازة وقاعد مبسوط وماسك تليفوني اهلي: النتيجه امتا يبو ملاحق', 'شمالي: تضامن مع قضية عبدالله الاغبري جنوبي: تضامن مع قضية عبدالله الاغبري فتحي بن لزرق: هذا دليل ان الشعب اليمني واحد وانه لا توجد تفرقة', 'عندي حوالي عشرغلطات في الورقة وانا كمان عندي غلطة في الورقة التانية ورقة تانية؟', 'لما حد يسألك إتعلمت إيه من زمايلك ف الشغل جعلوني مجرمآ', 'ياشيخ اقدر اصوم 60 يوم للضهر بدل 30 يوم للمغرب', 'يابنتي اللي في سنك فاتحين بيوت وعندهم عيال ربنا معاهم', 'صور فتيات الليل فى كندا سنة 1940 605 3٤ 05م طب دا الليل فين الفتيات ؟', 'أنتي بتروحي تعملي إيه ف الكليه أنا :', 'الدكتور قاله ابعد عن الشيشة', 'لما افتكراني متوضيتش وانا ف نص الميك اب']\n"
          ]
        }
      ],
      "source": [
        "print(test_df['processed_text'].to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q18vYra4tH3Z",
        "outputId": "f0a80b0b-4762-4c7a-88cc-3b109eba103e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with hate_label == 1: 1930\n"
          ]
        }
      ],
      "source": [
        "# Filter the DataFrame to count rows where 'hate_label' is 1\n",
        "hate_count = train_df[train_df['hate_label'] == 0].shape[0]\n",
        "print(f\"Number of rows with hate_label == 1: {hate_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SVM***"
      ],
      "metadata": {
        "id": "KVh7Fd6K5_Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Initialize CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "4QvVJ2rN59YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Extract CLIP Features with Logging\n",
        "def get_clip_features(text, img_path, meme_id):\n",
        "  base_dir = '/content/drive/MyDrive/Prop2Hate-Meme'  # Update with actual path\n",
        "  img_path = os.path.join(base_dir, img_path.lstrip('./'))\n",
        "  print(f\"Processing ID: {meme_id}, Text: {text}\")\n",
        "  try:\n",
        "      img = Image.open(img_path).convert('RGB')\n",
        "      inputs = clip_processor(text=[text], images=[img], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = clip_model(**inputs)\n",
        "      text_embedding = outputs.text_embeds.cpu().numpy()\n",
        "      image_embedding = outputs.image_embeds.cpu().numpy()\n",
        "      return np.concatenate([text_embedding, image_embedding], axis=1)[0]\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing ID: {meme_id}, Image: {img_path}: {e}\")\n",
        "      with open('/content/drive/MyDrive/Prop2Hate-Meme/error_log.txt', 'a', encoding='utf-8') as f:\n",
        "          f.write(f\"Error processing ID: {meme_id}, Image: {img_path}: {e}\\n\")\n",
        "      return np.zeros(1024)"
      ],
      "metadata": {
        "id": "DpMnLZyE6lYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features for all datasets\n",
        "print(\"Extracting features for training set...\")\n",
        "train_features = np.array([get_clip_features(text, img_path, meme_id)\n",
        "                          for text, img_path, meme_id in zip(train_df['processed_text'], train_df['img_path'], train_df['id'])])\n",
        "print(\"\\nExtracting features for dev set...\")\n",
        "dev_features = np.array([get_clip_features(text, img_path, meme_id)\n",
        "                         for text, img_path, meme_id in zip(dev_df['processed_text'], dev_df['img_path'], dev_df['id'])])\n",
        "print(\"\\nExtracting features for test set...\")\n",
        "test_features = np.array([get_clip_features(text, img_path, meme_id)\n",
        "                          for text, img_path, meme_id in zip(test_df['processed_text'], test_df['img_path'], test_df['id'])])"
      ],
      "metadata": {
        "id": "C9KC1bCF6oll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Labels\n",
        "train_labels = train_df['hate_label'].values\n",
        "dev_labels = dev_df['hate_label'].values\n",
        "test_labels = test_df['hate_label'].values"
      ],
      "metadata": {
        "id": "1JdmmQdf6rKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train SVM Classifier\n",
        "svm = SVC()\n",
        "svm.fit(train_features, train_labels)"
      ],
      "metadata": {
        "id": "SmQ6DYy86tfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate Metrics Function\n",
        "def evaluate_metrics(true_labels, pred_labels, dataset_name):\n",
        "    micro_f1 = f1_score(true_labels, pred_labels, average='micro')\n",
        "    precision = precision_score(true_labels, pred_labels, average='micro')\n",
        "    recall = recall_score(true_labels, pred_labels, average='micro')\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "    print(f\"\\n{dataset_name} Set Evaluation:\")\n",
        "    print(f\"Micro-F1 Score (Official Metric): {micro_f1:.4f}\")\n",
        "    print(f\"Micro Precision: {precision:.4f}\")\n",
        "    print(f\"Micro Recall: {recall:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"{dataset_name} Set Classification Report:\\n\", classification_report(true_labels, pred_labels))\n",
        "\n",
        "    # Save metrics to file in output directory\n",
        "    with open(os.path.join(output_dir, f'{dataset_name}_metrics.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"{dataset_name} Set Evaluation:\\n\")\n",
        "        f.write(f\"Micro-F1 Score (Official Metric): {micro_f1:.4f}\\n\")\n",
        "        f.write(f\"Micro Precision: {precision:.4f}\\n\")\n",
        "        f.write(f\"Micro Recall: {recall:.4f}\\n\")\n",
        "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
        "        f.write(f\"\\nClassification Report:\\n{classification_report(true_labels, pred_labels)}\")\n",
        "\n",
        "    return micro_f1, precision, recall, accuracy"
      ],
      "metadata": {
        "id": "hTW5luhF6v-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluate on Dev and Test Sets\n",
        "dev_pred = svm.predict(dev_features)\n",
        "evaluate_metrics(dev_labels, dev_pred, \"Dev\")\n",
        "\n",
        "test_pred = svm.predict(test_features)\n",
        "evaluate_metrics(test_labels, test_pred, \"Test\")"
      ],
      "metadata": {
        "id": "9ub8p8rE60Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqFwQ4hpHll1"
      },
      "source": [
        "# **ELUnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx6sw7Z4DwCx",
        "outputId": "5a05745e-1c9e-4e8f-9abe-e0ff571c36b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run folder: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/500_hateful_meme/hateful_meme_drive_50_500_samples_bicubic_l1_256_task3\n"
          ]
        }
      ],
      "source": [
        "# make run output folder in google drive\n",
        "LOG_DIR = \"/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet\"\n",
        "\n",
        "name = \"hateful_meme_drive_50_500_samples_bicubic_l1_256_task3\"\n",
        "run_folder = os.path.join(LOG_DIR, \"500_hateful_meme\", name)\n",
        "\n",
        "os.makedirs(run_folder, exist_ok=True)\n",
        "\n",
        "print(f'Run folder: {run_folder}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhstOe6ZHs5q"
      },
      "source": [
        "# **ELUnet Model Initialize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h-lWSb1B_M3c"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\" [(Conv2d) => (BN) => (ReLu)] * 2 \"\"\"\n",
        "\n",
        "    def __init__(self,in_channels,out_channels) -> None:\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,3,padding=\"same\",stride=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(out_channels,out_channels,3,padding=\"same\",stride=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "\n",
        "    \"\"\" MaxPool => DoubleConv \"\"\"\n",
        "    def __init__(self,in_channels,out_channels) -> None:\n",
        "        super().__init__()\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels,out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x  = self.down_sample(x)\n",
        "        return x\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,c:int) -> None:\n",
        "        \"\"\" UpSample input tensor by a factor of `c`\n",
        "                - the value of base 2 log c defines the number of upsample\n",
        "                layers that will be applied\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        n = 0 if c == 0 else int(math.log(c,2))\n",
        "\n",
        "        self.upsample = nn.ModuleList(\n",
        "            [nn.ConvTranspose2d(in_channels,in_channels,2,2) for i in range(n)]\n",
        "        )\n",
        "        self.conv_3 = nn.Conv2d(in_channels,out_channels,3,padding=\"same\",stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.upsample:\n",
        "            x = layer(x)\n",
        "        return self.conv_3(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.squeeze(-1).squeeze(-1)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-4b49uwV_M3c"
      },
      "outputs": [],
      "source": [
        "def cross_attention(Q, K, V):\n",
        "    # Compute the dot products between Q and K, then scale\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "    # Softmax to normalize scores and get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Weighted sum of values\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_size, text_embed_size):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.text_embed_size = text_embed_size\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(text_embed_size, embed_size)\n",
        "        self.value = nn.Linear(text_embed_size, embed_size)\n",
        "\n",
        "        # Final linear layer after concatenating heads\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, target, source):\n",
        "\n",
        "        Q = self.query(target)\n",
        "        K = self.key(source)\n",
        "        V = self.value(source)\n",
        "\n",
        "        # Perform attention calculation (self or cross)\n",
        "        out, _ = cross_attention(Q, K, V)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class CrossAttentionWithResidual(nn.Module):\n",
        "    def __init__(self, embed_size, text_embed_size):\n",
        "        super(CrossAttentionWithResidual, self).__init__()\n",
        "        self.embed_dim = embed_size\n",
        "        self.attention = CrossAttention(embed_size, text_embed_size)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, target, source):\n",
        "        batch_size, channels, height, width = target.shape\n",
        "\n",
        "        seq_len = height * width  # 4 * 4 = 16\n",
        "        target = target.permute(0, 2, 3, 1).reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        attention_out = self.attention(target, source)\n",
        "        # Add residual connection and layer normalization\n",
        "        out = self.norm(target + self.dropout(attention_out))\n",
        "\n",
        "        attn_output = out.view(batch_size, height, width, self.embed_dim).permute(0, 3, 1, 2)\n",
        "        return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8ENoUP1C_M3c"
      },
      "outputs": [],
      "source": [
        "class ELUnet(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,text_embed_size,n:int = 8) -> None:\n",
        "        \"\"\"\n",
        "        Construct the Elu-net model.\n",
        "        Args:\n",
        "            in_channels: The number of color channels of the input image. 0:for binary 3: for RGB\n",
        "            out_channels: The number of color channels of the input mask, corresponds to the number\n",
        "                            of classes.Includes the background\n",
        "            n: Channels size of the first CNN in the encoder layer. The bigger this value the bigger\n",
        "                the number of parameters of the model. Defaults to n = 8, which is recommended by the\n",
        "                authors of the paper.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.const = n\n",
        "\n",
        "        # ------ Input convolution --------------\n",
        "        self.in_conv = DoubleConv(in_channels,n)\n",
        "        # -------- Encoder ----------------------\n",
        "        self.down_1 = DownSample(n, 2*n)\n",
        "        self.down_2 = DownSample(2*n, 4*n)\n",
        "        self.down_3 = DownSample(4*n, 8*n)\n",
        "        self.down_4 = DownSample(8*n, 16*n)\n",
        "\n",
        "        # ------ Cross Attention --------------\n",
        "        self.model_attention = CrossAttentionWithResidual(16*n, text_embed_size)\n",
        "\n",
        "        # -------- Upsampling ------------------\n",
        "        self.up_1024_512 = UpSample(16*n, 8*n, 2)\n",
        "\n",
        "        self.up_512_64 = UpSample(8*n, n, 8)\n",
        "        self.up_512_128 = UpSample(8*n, 2*n, 4)\n",
        "        self.up_512_256 = UpSample(8*n, 4*n, 2)\n",
        "        self.up_512_512 = UpSample(8*n, 8*n, 0)\n",
        "\n",
        "        self.up_256_64 = UpSample(4*n, n, 4)\n",
        "        self.up_256_128 = UpSample(4*n, 2*n, 2)\n",
        "        self.up_256_256 = UpSample(4*n, 4*n, 0)\n",
        "\n",
        "        self.up_128_64 = UpSample(2*n, n, 2)\n",
        "        self.up_128_128 = UpSample(2*n, 2*n, 0)\n",
        "\n",
        "        self.up_64_64 = UpSample(n, n, 0)\n",
        "\n",
        "        # ------ Decoder block ---------------\n",
        "        self.dec_4 = DoubleConv(2*8*n,8*n)\n",
        "        self.dec_3 = DoubleConv(3*4*n,4*n)\n",
        "        self.dec_2 = DoubleConv(4*2*n,2*n)\n",
        "        self.dec_1 = DoubleConv(5*n,n)\n",
        "        # ------ Output convolution\n",
        "\n",
        "        self.out_conv = OutConv(n,out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x, text_embeddings):\n",
        "\n",
        "        x = self.in_conv(x) # 64\n",
        "\n",
        "        # ---- Encoder outputs\n",
        "        x_enc_1 = self.down_1(x) # 128\n",
        "        x_enc_2 = self.down_2(x_enc_1) # 256\n",
        "        x_enc_3 = self.down_3(x_enc_2) # 512\n",
        "        x_enc_4 = self.down_4(x_enc_3) # 1024\n",
        "\n",
        "        # ------ Cross Attention layer\n",
        "        attention_result = self.model_attention(x_enc_4, text_embeddings)\n",
        "\n",
        "        # ------ decoder outputs\n",
        "        x_up_1 = self.up_1024_512(attention_result)\n",
        "        x_dec_4 = self.dec_4(torch.cat([x_up_1,self.up_512_512(x_enc_3)],dim=1))\n",
        "\n",
        "        x_up_2 = self.up_512_256(x_dec_4)\n",
        "        x_dec_3 = self.dec_3(torch.cat([x_up_2,\n",
        "            self.up_512_256(x_enc_3),\n",
        "            self.up_256_256(x_enc_2)\n",
        "            ],\n",
        "        dim=1))\n",
        "\n",
        "        x_up_3 = self.up_256_128(x_dec_3)\n",
        "        x_dec_2 = self.dec_2(torch.cat([\n",
        "            x_up_3,\n",
        "            self.up_512_128(x_enc_3),\n",
        "            self.up_256_128(x_enc_2),\n",
        "            self.up_128_128(x_enc_1)\n",
        "        ],dim=1))\n",
        "\n",
        "        x_up_4 = self.up_128_64(x_dec_2)\n",
        "        x_dec_1 = self.dec_1(torch.cat([\n",
        "            x_up_4,\n",
        "            self.up_512_64(x_enc_3),\n",
        "            self.up_256_64(x_enc_2),\n",
        "            self.up_128_64(x_enc_1),\n",
        "            self.up_64_64(x)\n",
        "        ],dim=1))\n",
        "\n",
        "        return self.out_conv(x_dec_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CiWqHXD7_M3c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "elunet = ELUnet(in_channels=3, out_channels=1, text_embed_size=256)\n",
        "elunet = elunet.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "08eIzoaC_M3d"
      },
      "outputs": [],
      "source": [
        "temp1 = torch.rand(1, 3, 256, 256).to(device) # Add a batch dimension\n",
        "temp2 = torch.rand(1, 256).to(device) # Add a batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuIlnLfj_M3d",
        "outputId": "5443dd8e-6d1e-4d19-d12a-13953b1c5d5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "result = elunet(temp1, temp2)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAnjcKI4_M3d"
      },
      "source": [
        "# **Distil Bert Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "svx3jjQF_M3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedc28bb-521c-4b1b-e6e9-862a14027296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# model_name = 'aubmindlab/bert-base-arabertv2'\n",
        "\n",
        "model_name = 'distilbert-base-multilingual-cased'\n",
        "fixed_length = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        "    use_fast=True,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "# text_model = AutoModel.from_pretrained(\n",
        "#     model_name,\n",
        "#     cache_dir=None,\n",
        "#     revision=\"main\",\n",
        "#     use_auth_token=None,\n",
        "# ).eval()\n",
        "\n",
        "def tokenize_fixed_length(text):\n",
        "    \"\"\"\n",
        "    Tokenize Arabic text to a fixed length with non-zero input_ids.\n",
        "    Repeats non-special tokens if the text is too short.\n",
        "    \"\"\"\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',  # No padding initially\n",
        "        truncation=True,  # Truncate if too long\n",
        "        max_length=fixed_length,  # Ensure we don't exceed fixed_length\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = encoding['input_ids'][0]  # Shape: (seq_len,)\n",
        "    attention_mask = encoding['attention_mask'][0]\n",
        "\n",
        "    # Get non-special tokens (exclude [CLS] and [SEP])\n",
        "    non_special_ids = input_ids[attention_mask == 1][1:-1]  # Exclude [CLS] (101) and [SEP] (102)\n",
        "\n",
        "    if len(non_special_ids) == 0:\n",
        "        new_input_ids = [101] + [100] * (fixed_length - 2) + [102]\n",
        "    else:\n",
        "        repeat_count = (fixed_length - 2) // len(non_special_ids) + 1\n",
        "        repeated_ids = (non_special_ids.repeat(repeat_count)[:fixed_length - 2]).tolist()\n",
        "        new_input_ids = [101] + repeated_ids + [102]\n",
        "        new_input_ids = new_input_ids + [new_input_ids[-2]] * (fixed_length - len(new_input_ids))\n",
        "\n",
        "    new_input_ids = new_input_ids[:fixed_length]\n",
        "    new_attention_mask = [1] * fixed_length\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor([new_input_ids]),\n",
        "        'attention_mask': torch.tensor([new_attention_mask])\n",
        "    }\n",
        "\n",
        "def tokenize_fixed_length_2(text):\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',  #No padding initially\n",
        "        truncation=True,  # Truncate if too long\n",
        "        max_length=fixed_length,  # Ensure we don't exceed fixed_length\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Extract input_ids and attention_mask\n",
        "    input_ids = encoding['input_ids'][0]  # Shape: (seq_len,)\n",
        "    attention_mask = encoding['attention_mask'][0]\n",
        "\n",
        "    # Get non-special tokens (exclude [CLS] and [SEP])\n",
        "    non_special_ids = input_ids[attention_mask == 1][1:-1]  # Exclude [CLS] (101) and [SEP] (102)\n",
        "\n",
        "    if len(non_special_ids) == 0:\n",
        "        new_input_ids = [101] + [100] * (fixed_length - 2) + [102]\n",
        "    elif len(non_special_ids) > fixed_length:\n",
        "        pca = PCA(n_components=fixed_length-2)\n",
        "        result = pca.fit_transform(non_special_ids)\n",
        "        new_input_ids = [101] + result + [102]\n",
        "    else:\n",
        "        repeat_count = (fixed_length - 2) // len(non_special_ids) + 1\n",
        "        repeated_ids = (non_special_ids.repeat(repeat_count)[:fixed_length - 2]).tolist()\n",
        "        new_input_ids = [101] + repeated_ids + [102]\n",
        "        new_input_ids = new_input_ids + [new_input_ids[-2]] * (fixed_length - len(new_input_ids))\n",
        "\n",
        "    new_input_ids = new_input_ids[:fixed_length]\n",
        "    new_attention_mask = [1] * fixed_length\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor([new_input_ids]),\n",
        "        'attention_mask': torch.tensor([new_attention_mask])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZGE__bF_M3d",
        "outputId": "2971b99e-7aa8-4059-8613-a107879328ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 101, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,  195, 9950,\n",
            "         195, 9950,  195,  102])\n"
          ]
        }
      ],
      "source": [
        "short_text = \"مرحبا\"\n",
        "vector1 = tokenize_fixed_length_2(short_text)\n",
        "print(vector1['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iNmAQf9IiR_"
      },
      "source": [
        "# **Custom Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YhD884UilieQ"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        self.df = df  # Use provided train_df DataFrame\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.skipped_samples = []  # Initialize skipped_samples list\n",
        "\n",
        "        #print(f\"Labels in DataFrame: {self.df['hate_label'].unique()}\")\n",
        "        print(f\"Loaded {len(self.df)} rows\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        id = row['id']\n",
        "        image_name = row['img_path']\n",
        "        image_path = os.path.join(self.image_dir, image_name.lstrip('./'))\n",
        "        label = torch.tensor([row['hate_label']])\n",
        "        text = row.get('processed_text', '')\n",
        "        embedding = tokenize_fixed_length_2(text)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {row['id']} from {image_path}: {e}\")\n",
        "            self.skipped_samples.append({\n",
        "                'index': row['id'],\n",
        "                'img_path': image_path,\n",
        "                'reason': f'Error loading image: {e}'\n",
        "            })\n",
        "            image = torch.zeros(3, 256, 256)  # Placeholder for failed images\n",
        "        # return {\n",
        "        #     'id': id,\n",
        "        #     'image': image,\n",
        "        #     'embedding': embedding\n",
        "        # }\n",
        "\n",
        "        return image, embedding, label  # Return only image and label\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ZAbtftA67v",
        "outputId": "c1336167-9dc7-4a42-958c-3a36fad042fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2143 rows\n",
            "Loaded 2143 images\n"
          ]
        }
      ],
      "source": [
        "size = 256\n",
        "transform = torch_transforms.Compose([\n",
        "        torch_transforms.Resize(size, interpolation=InterpolationMode.BICUBIC),\n",
        "        torch_transforms.CenterCrop(size),\n",
        "        _convert_image_to_rgb,\n",
        "        torch_transforms.ToTensor(),\n",
        "        torch_transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "image_dir = \"/content/drive/MyDrive/Prop2Hate-Meme/\"  # Update to your image folder\n",
        "\n",
        "target_dataset = CustomImageDataset(train_df, image_dir, transform=transform)\n",
        "\n",
        "print(f\"Loaded {len(target_dataset)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcAHPTxE_M3e",
        "outputId": "f29620e9-43f0-45b0-fbec-8925705c7447"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "target_dataset[1][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8pmQBMgd_M3e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights(dataset):\n",
        "    \"\"\"\n",
        "    Compute pos_weight for BCEWithLogitsLoss based on class distribution.\n",
        "\n",
        "    Args:\n",
        "        dataset: Training dataset with labels\n",
        "    Returns:\n",
        "        pos_weight: Scalar for BCEWithLogitsLoss\n",
        "    \"\"\"\n",
        "    labels = [dataset[i][2].item() for i in range(len(dataset))]  #  label is at index 2\n",
        "    num_positive = sum(labels)\n",
        "    num_negative = len(labels) - num_positive\n",
        "    pos_weight = num_negative / max(num_positive, 1)  # Avoid division by zero\n",
        "    return torch.tensor([pos_weight], dtype=torch.float)"
      ],
      "metadata": {
        "id": "JIS4srdrEj35"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_sampler(dataset):\n",
        "    \"\"\"\n",
        "    Create a WeightedRandomSampler for balanced sampling.\n",
        "\n",
        "    Args:\n",
        "        dataset: Training dataset with labels\n",
        "    Returns:\n",
        "        sampler: WeightedRandomSampler for DataLoader\n",
        "    \"\"\"\n",
        "    labels = [dataset[i][2].item() for i in range(len(dataset))]\n",
        "    class_counts = Counter(labels)\n",
        "    weights = [1.0 / class_counts[label] for label in labels]\n",
        "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "    return sampler"
      ],
      "metadata": {
        "id": "B7owrMRxEwN8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNEGqqh2_M3e"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOLEA15v_M3f",
        "outputId": "7e41891f-c4f4-4ac7-9240-9476574b7429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2143 rows\n",
            "Loaded 312 rows\n",
            "Loaded 606 rows\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "# frames = [train_df, dev_df]\n",
        "# df = pd.concat(frames, ignore_index=True)\n",
        "train_dataset = CustomImageDataset(train_df, image_dir, transform=transform)\n",
        "dev_dataset = CustomImageDataset(dev_df, image_dir, transform=transform)\n",
        "test_dataset = CustomImageDataset(test_df, image_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Y7pNkAUh_M3f"
      },
      "outputs": [],
      "source": [
        "def train_model(model,\n",
        "                train_dataset,\n",
        "                val_dataset,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                num_epochs,\n",
        "                device,\n",
        "                patience=3,\n",
        "                batch_size=16,\n",
        "                min_delta=0.001,\n",
        "                use_weighted_sampler=False):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create DataLoader with optional weighted sampling\n",
        "    if use_weighted_sampler:\n",
        "        sampler = get_weighted_sampler(train_dataset)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    # DataLoader for batching\n",
        "    #train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
        "    # criterion = nn.MSELoss()\n",
        "    # optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = os.path.join(output_dir, 'elunet_best_model_distilBert.pth')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "          images, texts_dict, labels = batch # Changed variable name to reflect that it's a dictionary\n",
        "          images = images.to(device)\n",
        "          texts = texts_dict['input_ids'].to(device).float() # Extract 'input_ids' and move to device, convert to float\n",
        "          labels = labels.to(device).float()  # Assuming binary labels (0 or 1)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images, texts)  # Removed .squeeze()\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item() * images.size(0)\n",
        "          train_total += images.size(0)\n",
        "        avg_train_loss = train_loss / train_total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "          for batch in tqdm.tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, texts_dict, labels = batch # Changed variable name to reflect that it's a dictionary\n",
        "            images = images.to(device)\n",
        "            texts = texts_dict['input_ids'].to(device).float() # Extract 'input_ids' and move to device, convert to float\n",
        "            labels = labels.to(device).float()\n",
        "            outputs = model(images, texts) # Removed .squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_total += images.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / val_total\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss - min_delta:\n",
        "          best_val_loss = avg_val_loss\n",
        "          patience_counter = 0\n",
        "          # Save model weights\n",
        "          torch.save(model.state_dict(), best_model_path)\n",
        "          print(f\"Saved best model weights to: {best_model_path}\")\n",
        "        else:\n",
        "          patience_counter += 1\n",
        "          print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
        "          if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model weights before returning\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(f\"Loaded best model weights from: {best_model_path}\")\n",
        "    return model\n",
        "\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     total_loss = 0\n",
        "    #     total = 0\n",
        "    #     correct = 0\n",
        "    #     for batch in train_loader:\n",
        "    #       #print('1')\n",
        "    #       images, text_embedding, target = batch\n",
        "\n",
        "    #       images = images.to(device).float()\n",
        "    #       text_embedding = text_embedding['input_ids'][0].to(device).float()\n",
        "    #       target = target.to(device).float()\n",
        "\n",
        "    #       # Forward pass\n",
        "    #       outputs = model(images, text_embedding)\n",
        "    #       loss = criterion(outputs, target)\n",
        "\n",
        "    #       # Backward pass and optimization\n",
        "    #       optimizer.zero_grad()\n",
        "    #       loss.backward()\n",
        "    #       optimizer.step()\n",
        "\n",
        "    #       predicted = (torch.sigmoid(outputs) >= 0.5).float()\n",
        "    #       correct += (predicted == target.view(-1, 1)).sum().item()\n",
        "    #       total += target.size(0)\n",
        "    #       total_loss += loss.item()\n",
        "\n",
        "    #     # Print epoch loss\n",
        "    #     avg_loss = total_loss / len(train_loader)\n",
        "    #     accuracy = 100 * correct / total\n",
        "    #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Train the model\n",
        "#train_model(model=elunet, train_dataset=target_dataset, batch_size=16, num_epochs=15, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(model, test_dataset, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set and save classification report to JSON.\n",
        "\n",
        "    Args:\n",
        "        model: Trained ELUnet model\n",
        "        test_loader: DataLoader for test data\n",
        "        device: Device to run evaluation on ('cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_data = []\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False) # Set shuffle to False\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm.tqdm(test_loader, desc=\"Testing\")): # Enumerate to get batch index\n",
        "            images, texts_dict, labels = batch\n",
        "            images = images.to(device)\n",
        "            texts = texts_dict['input_ids'].to(device).float() # Extract 'input_ids' and move to device, convert to float\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(images, texts).squeeze()\n",
        "            preds = (outputs > 0.5).float()  # Threshold for binary classification\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Collect dataset-specific fields (id, text, img_path, hate_label) and correctness\n",
        "            for i in range(len(labels)):\n",
        "                # Calculate the original index in the dataset\n",
        "                original_idx = batch_idx * test_loader.batch_size + i\n",
        "                if original_idx < len(test_dataset): # Ensure index is within dataset bounds\n",
        "                    sample = test_dataset[original_idx]\n",
        "                    # Check if sample is not None (in case image loading failed)\n",
        "                    if sample is not None:\n",
        "                        true_label = int(labels[i].cpu().numpy())\n",
        "                        pred_label = int(preds[i].cpu().numpy())\n",
        "                        all_data.append({\n",
        "                            'id': test_dataset.df.iloc[original_idx]['id'], # Access id from original dataframe\n",
        "                            'text': test_dataset.df.iloc[original_idx]['text'], # Access text from original dataframe\n",
        "                            'img_path': test_dataset.df.iloc[original_idx]['img_path'], # Access img_path from original dataframe\n",
        "                            'hate_label': true_label,  # True label\n",
        "                            'predicted_label': pred_label,  # Predicted label\n",
        "                            'correct': true_label == pred_label  # True if prediction matches true label, False otherwise\n",
        "                        })\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    class_report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Save classification report to JSON\n",
        "    report_path = os.path.join(output_dir, 'classification_report_with_weight_distilBert.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(class_report, f, indent=4)\n",
        "\n",
        "    # Save predictions, dataset columns, and correctness to CSV\n",
        "    predictions_path = os.path.join(output_dir, 'predictions_with_labels_with_weight_distilBert.csv')\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(predictions_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Classification Report saved to: {report_path},{predictions_path}\")\n",
        "\n",
        "    return accuracy, macro_f1, precision, recall, f1"
      ],
      "metadata": {
        "id": "5T6ZORHNAnaG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ZYKAEpHP6P",
        "outputId": "b1e784be-faa8-4fd2-c950-86232fe88fd0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = elunet.to(device)\n",
        "# pos_weight = compute_class_weights(train_dataset)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Adjust learning rate as needed\n",
        "num_epochs = 10\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "min_delta = 0.001  # Minimum improvement in validation loss"
      ],
      "metadata": {
        "id": "JFU0YW-YJQEm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "model = elunet.to(device)"
      ],
      "metadata": {
        "id": "lYliOlUbjz9p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train_model(model, train_dataset, dev_dataset, criterion, optimizer, num_epochs, device, patience, batch_size=16, min_delta=min_delta, use_weighted_sampler=False)\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcC-ia7YBxY-",
        "outputId": "f91b4ac4-686e-4091-a7fa-b5e1e2fd1286"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 67/67 [01:45<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 0.8662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:07<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Validation Loss: 0.8435\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 67/67 [01:33<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Training Loss: 0.8291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Validation Loss: 0.8143\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Training Loss: 0.7996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Validation Loss: 0.7915\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Training Loss: 0.7721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Validation Loss: 0.7640\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Training Loss: 0.7509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Validation Loss: 0.7424\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 - Training Loss: 0.7356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 - Validation Loss: 0.7289\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 - Training Loss: 0.7246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 - Validation Loss: 0.7209\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 - Training Loss: 0.7170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 - Validation Loss: 0.7148\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 - Training Loss: 0.7113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 - Validation Loss: 0.7085\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 - Training Loss: 0.7069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 - Validation Loss: 0.7055\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 - Training Loss: 0.7036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 - Validation Loss: 0.7028\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 - Training Loss: 0.7013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 - Validation Loss: 0.6999\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Training Loss: 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Validation Loss: 0.6986\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 - Training Loss: 0.6983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 - Validation Loss: 0.6978\n",
            "No improvement in validation loss. Patience counter: 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 - Training Loss: 0.6973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 - Validation Loss: 0.6965\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 - Training Loss: 0.6965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 - Validation Loss: 0.6963\n",
            "No improvement in validation loss. Patience counter: 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 67/67 [01:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 - Training Loss: 0.6959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 - Validation Loss: 0.6958\n",
            "No improvement in validation loss. Patience counter: 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 - Training Loss: 0.6954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 - Validation Loss: 0.6954\n",
            "Saved best model weights to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 - Training Loss: 0.6951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 - Validation Loss: 0.6953\n",
            "No improvement in validation loss. Patience counter: 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 67/67 [01:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 - Training Loss: 0.6949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:08<00:00,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 - Validation Loss: 0.6949\n",
            "No improvement in validation loss. Patience counter: 2/3\n",
            "Loaded best model weights from: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "elunet = ELUnet(in_channels=3, out_channels=1, text_embed_size=256)\n",
        "model = elunet\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/elunet_best_model_distilBert.pth', weights_only=True))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVTH0zqzpTEa",
        "outputId": "659291dc-7f11-45fc-bd43-3b00be8db1ea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ELUnet(\n",
              "  (in_conv): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (down_1): DownSample(\n",
              "    (down_sample): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down_2): DownSample(\n",
              "    (down_sample): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down_3): DownSample(\n",
              "    (down_sample): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down_4): DownSample(\n",
              "    (down_sample): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (model_attention): CrossAttentionWithResidual(\n",
              "    (attention): CrossAttention(\n",
              "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (up_1024_512): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_512_64): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0-2): 3 x ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_512_128): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0-1): 2 x ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_512_256): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_512_512): UpSample(\n",
              "    (upsample): ModuleList()\n",
              "    (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_256_64): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0-1): 2 x ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_256_128): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_256_256): UpSample(\n",
              "    (upsample): ModuleList()\n",
              "    (conv_3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_128_64): UpSample(\n",
              "    (upsample): ModuleList(\n",
              "      (0): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (conv_3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_128_128): UpSample(\n",
              "    (upsample): ModuleList()\n",
              "    (conv_3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (up_64_64): UpSample(\n",
              "    (upsample): ModuleList()\n",
              "    (conv_3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  )\n",
              "  (dec_4): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (dec_3): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (dec_2): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (dec_1): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(40, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (out_conv): OutConv(\n",
              "    (conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (pool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "7lkNlSg0VRe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "lGTyyPYnx7p4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_metrics(model, test_dataset, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-cNGS1npWlC",
        "outputId": "dcb70aba-9b62-4827-c73c-5fafaddf09bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:   0%|          | 0/152 [00:00<?, ?it/s]/tmp/ipython-input-2603053714.py:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  true_label = int(labels[i].cpu().numpy())\n",
            "Testing: 100%|██████████| 152/152 [00:27<00:00,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7459\n",
            "Macro F1 Score: 0.4272\n",
            "Precision: 0.3729\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.4272\n",
            "Classification Report saved to: /content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/classification_report_without_weight_araBert.json,/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/predictions_with_labels_without_weight_distilBert.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7458745874587459,\n",
              " 0.42722117202268434,\n",
              " 0.37293729372937295,\n",
              " 0.5,\n",
              " 0.42722117202268434)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the predictions from the CSV file\n",
        "predictions_df = pd.read_csv('/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/predictions.csv')\n",
        "\n",
        "# Check if the 'id' columns are the same\n",
        "if predictions_df['id'].equals(test_df['id']):\n",
        "    print(\"The 'id' columns are in the same order.\")\n",
        "else:\n",
        "    print(\"The 'id' columns are not in the same order.\")\n",
        "    # Find and print the differences\n",
        "    for i, (pred_id, test_id) in enumerate(zip(predictions_df['id'], test_df['id'])):\n",
        "        if pred_id != test_id:\n",
        "            print(f\"Mismatch at index {i}: prediction_id='{pred_id}', test_id='{test_id}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5TXjVSlYTcwW",
        "outputId": "8c3e6cc6-4f77-43f8-fafd-069961f9618a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/predictions.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2921660459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the predictions from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check if the 'id' columns are the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Prop2Hate-Meme/Result/Elunet/WithOut_Diff/predictions.csv'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}